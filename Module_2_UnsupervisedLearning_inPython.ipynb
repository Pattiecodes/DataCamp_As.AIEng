{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuMpJ3ZoGh85bFgGcBkbxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_2_UnsupervisedLearning_inPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning in Python. Part II of the Associate AI Engineer for Data Scientists Career Track"
      ],
      "metadata": {
        "id": "XLXEiZfpZdPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering 2D points\n",
        "\n",
        "From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.\n",
        "\n",
        "You are given the array points from the previous exercise, and also an array new_points.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import KMeans from sklearn.cluster.\n",
        "Using KMeans(), create a KMeans instance called model to find 3 clusters. To specify the number of clusters, use the n_clusters keyword argument.\n",
        "Use the .fit() method of model to fit the model to the array of points points.\n",
        "Use the .predict() method of model to predict the cluster labels of new_points, assigning the result to labels.\n",
        "Hit submit to see the cluster labels of new_points."
      ],
      "metadata": {
        "id": "Ng3x9lfVZtHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import KMeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a KMeans instance with 3 clusters: model\n",
        "model = KMeans(n_clusters=3)\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(points)\n",
        "\n",
        "# Determine the cluster labels of new_points: labels\n",
        "labels = model.predict(new_points)\n",
        "\n",
        "# Print cluster labels of new_points\n",
        "print(labels)\n"
      ],
      "metadata": {
        "id": "SyIE1IIjS8H2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect your clustering\n",
        "Let's now inspect the clustering you performed in the previous exercise!\n",
        "\n",
        "A solution to the previous exercise has already run, so new_points is an array of points and labels is the array of their cluster labels.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import matplotlib.pyplot as plt.\n",
        "Assign column 0 of new_points to xs, and column 1 of new_points to ys.\n",
        "Make a scatter plot of xs and ys, specifying the c=labels keyword arguments to color the points by their cluster label. Also specify alpha=0.5.\n",
        "Compute the coordinates of the centroids using the .cluster_centers_ attribute of model.\n",
        "Assign column 0 of centroids to centroids_x, and column 1 of centroids to centroids_y.\n",
        "Make a scatter plot of centroids_x and centroids_y, using 'D' (a diamond) as a marker by specifying the marker parameter. Set the size of the markers to be 50 using s=50."
      ],
      "metadata": {
        "id": "6g79Q1f4UoOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assign the columns of new_points: xs and ys\n",
        "xs = new_points[:,0]\n",
        "ys = new_points[:,1]\n",
        "\n",
        "# Make a scatter plot of xs and ys, using labels to define the colors\n",
        "plt.scatter(xs, ys, c=labels, alpha=0.5)\n",
        "\n",
        "# Assign the cluster centers: centroids\n",
        "centroids = model.cluster_centers_\n",
        "\n",
        "# Assign the columns of centroids: centroids_x, centroids_y\n",
        "centroids_x = centroids[:,0]\n",
        "centroids_y = centroids[:,1]\n",
        "\n",
        "# Make a scatter plot of centroids_x and centroids_y\n",
        "plt.scatter(centroids_x, centroids_y, marker='D', s=50)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tsY5hLxTUqOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How many clusters of grain?\n",
        "\n",
        "In the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array samples containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?\n",
        "\n",
        "KMeans and PyPlot (plt) have already been imported for you.\n",
        "\n",
        "This dataset was sourced from the UCI Machine Learning Repository (https://archive.ics.uci.edu/dataset/236/seeds).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "For each of the given values of k, perform the following steps:\n",
        "Create a KMeans instance called model with k clusters.\n",
        "Fit the model to the grain data samples.\n",
        "Append the value of the inertia_ attribute of model to the list inertias.\n",
        "The code to plot ks vs inertias has been written for you, so hit submit to see the plot!"
      ],
      "metadata": {
        "id": "S8WTy_-G9cAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ks = range(1, 6)\n",
        "inertias = []\n",
        "\n",
        "for k in ks:\n",
        "    # Create a KMeans instance with k clusters: model\n",
        "    model = KMeans(n_clusters=k)\n",
        "\n",
        "    # Fit model to samples\n",
        "    model.fit(samples)\n",
        "\n",
        "    # Append the inertia to the list of inertias\n",
        "    inertias.append(model.inertia_)\n",
        "\n",
        "# Plot ks vs inertias\n",
        "plt.plot(ks, inertias, '-o')\n",
        "plt.xlabel('number of clusters, k')\n",
        "plt.ylabel('inertia')\n",
        "plt.xticks(ks)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bTs9MR6h9hOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the grain clustering\n",
        "\n",
        "In the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: \"Kama\", \"Rosa\" and \"Canadian\". In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.\n",
        "\n",
        "You have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (pd) and KMeans have already been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a KMeans model called model with 3 clusters.\n",
        "Use the .fit_predict() method of model to fit it to samples and derive the cluster labels. Using .fit_predict() is the same as using .fit() followed by .predict().\n",
        "Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.\n",
        "Use the pd.crosstab() function on df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label. Assign the result to ct.\n",
        "Hit submit to see the cross-tabulation!"
      ],
      "metadata": {
        "id": "Rq8q_l7T-fwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a KMeans model with 3 clusters: model\n",
        "model = KMeans(n_clusters=3)\n",
        "\n",
        "# Use fit_predict to fit model and obtain cluster labels: labels\n",
        "labels = model.fit_predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "metadata": {
        "id": "Z1W5fjz3-iGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling fish data for clustering\n",
        "You are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.\n",
        "\n",
        "These fish measurement data were sourced from the Journal of Statistics Education (https://jse.amstat.org/jse_data_archive.htm).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "make_pipeline from sklearn.pipeline.\n",
        "StandardScaler from sklearn.preprocessing.\n",
        "KMeans from sklearn.cluster.\n",
        "Create an instance of StandardScaler called scaler.\n",
        "Create an instance of KMeans with 4 clusters called kmeans.\n",
        "Create a pipeline called pipeline that chains scaler and kmeans. To do this, you just need to pass them in as arguments to make_pipeline()."
      ],
      "metadata": {
        "id": "CQvjqrAbmA9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create KMeans instance: kmeans\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, kmeans)\n"
      ],
      "metadata": {
        "id": "T3gxouSOmI71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering the fish data\n",
        "\n",
        "You'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.\n",
        "\n",
        "As before, samples is the 2D array of fish measurements. Your pipeline is available as pipeline, and the species of every fish sample is given by the list species.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Fit the pipeline to the fish measurements samples.\n",
        "Obtain the cluster labels for samples by using the .predict() method of pipeline.\n",
        "Using pd.DataFrame(), create a DataFrame df with two columns named 'labels' and 'species', using labels and species, respectively, for the column values.\n",
        "Using pd.crosstab(), create a cross-tabulation ct of df['labels'] and df['species']."
      ],
      "metadata": {
        "id": "gSCi7iT5mz5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Fit the pipeline to samples\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Calculate the cluster labels: labels\n",
        "labels = pipeline.predict(samples)\n",
        "\n",
        "# Create a DataFrame with labels and species as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'species': species})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['species'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)"
      ],
      "metadata": {
        "id": "XSAhNbTsm3e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering stocks using KMeans\n",
        "In this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.\n",
        "\n",
        "Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n",
        "\n",
        "Note that Normalizer() is different to StandardScaler(), which you used in the previous exercise. While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, Normalizer() rescales each sample - here, each company's stock price - independently of the other.\n",
        "\n",
        "KMeans and make_pipeline have already been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import Normalizer from sklearn.preprocessing.\n",
        "Create an instance of Normalizer called normalizer.\n",
        "Create an instance of KMeans called kmeans with 10 clusters.\n",
        "Using make_pipeline(), create a pipeline called pipeline that chains normalizer and kmeans.\n",
        "Fit the pipeline to the movements array."
      ],
      "metadata": {
        "id": "lqIJcBhJw9LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Normalizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create a normalizer: normalizer\n",
        "normalizer = Normalizer()\n",
        "\n",
        "# Create a KMeans model with 10 clusters: kmeans\n",
        "kmeans = KMeans(n_clusters=10)\n",
        "\n",
        "# Make a pipeline chaining normalizer and kmeans: pipeline\n",
        "pipeline = make_pipeline(normalizer, kmeans)\n",
        "\n",
        "# Fit pipeline to the daily price movements\n",
        "pipeline.fit(movements)\n"
      ],
      "metadata": {
        "id": "Ft5Buwj1xEPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Which stocks move together?\n",
        "In the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out.\n",
        "\n",
        "Your solution to the previous exercise has already been run. Recall that you constructed a Pipeline pipeline containing a KMeans model and fit it to the NumPy array movements of daily stock movements. In addition, a list companies of the company names is available.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Use the .predict() method of the pipeline to predict the labels for movements.\n",
        "Align the cluster labels with the list of company names companies by creating a DataFrame df with labels and companies as columns. This has been done for you.\n",
        "Use the .sort_values() method of df to sort the DataFrame by the 'labels' column, and print the result.\n",
        "Hit submit and take a moment to see which companies are together in each cluster!"
      ],
      "metadata": {
        "id": "weQBIEGixcfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Predict the cluster labels: labels\n",
        "labels = pipeline.predict(movements)\n",
        "\n",
        "# Create a DataFrame aligning labels and companies: df\n",
        "df = pd.DataFrame({'labels': labels, 'companies': companies})\n",
        "\n",
        "# Display df sorted by cluster label\n",
        "print(df.sort_values('labels'))\n"
      ],
      "metadata": {
        "id": "YWB8dRXWxgo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical clustering of the grain data\n",
        "\n",
        "In the video, you learned that the SciPy linkage() function performs hierarchical clustering on an array of samples. Use the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to visualize the result. A sample of the grain measurements is provided in the array samples, while the variety of each grain sample is given by the list varieties.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "linkage and dendrogram from scipy.cluster.hierarchy.\n",
        "matplotlib.pyplot as plt.\n",
        "Perform hierarchical clustering on samples using the linkage() function with the method='complete' keyword argument. Assign the result to mergings.\n",
        "Plot a dendrogram using the dendrogram() function on mergings. Specify the keyword arguments labels=varieties, leaf_rotation=90, and leaf_font_size=6."
      ],
      "metadata": {
        "id": "_1I-fFhH3MEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='complete')\n",
        "\n",
        "# Plot the dendrogram, using varieties as labels\n",
        "dendrogram(mergings,\n",
        "           labels=varieties,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6,\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "srW8CrvF3PJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchies of stocks\n",
        "In chapter 1, you used k-means clustering to cluster companies according to their stock price movements. Now, you'll perform hierarchical clustering of the companies. You are given a NumPy array of price movements movements, where the rows correspond to companies, and a list of the company names companies. SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so you'll need to use the normalize() function from sklearn.preprocessing instead of Normalizer.\n",
        "\n",
        "linkage and dendrogram have already been imported from scipy.cluster.hierarchy, and PyPlot has been imported as plt.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import normalize from sklearn.preprocessing.\n",
        "Rescale the price movements for each stock by using the normalize() function on movements.\n",
        "Apply the linkage() function to normalized_movements, using 'complete' linkage, to calculate the hierarchical clustering. Assign the result to mergings.\n",
        "Plot a dendrogram of the hierarchical clustering, using the list companies of company names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you did in the previous exercise."
      ],
      "metadata": {
        "id": "obpzAXuu39zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import normalize\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Normalize the movements: normalized_movements\n",
        "normalized_movements = normalize(movements)\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(normalized_movements, method='complete')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(mergings,\n",
        "           labels=companies,\n",
        "           leaf_rotation=90,\n",
        "           leaf_font_size=6\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GxfD2GPV4AsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Different linkage, different hierarchical clustering!\n",
        "In the video, you saw a hierarchical clustering of the voting countries at the Eurovision song contest using 'complete' linkage. Now, perform a hierarchical clustering of the voting countries with 'single' linkage, and compare the resulting dendrogram with the one in the video. Different linkage, different hierarchical clustering!\n",
        "\n",
        "You are given an array samples. Each row corresponds to a voting country, and each column corresponds to a performance that was voted for. The list country_names gives the name of each voting country. This dataset was obtained from Eurovision.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import linkage and dendrogram from scipy.cluster.hierarchy.\n",
        "Perform hierarchical clustering on samples using the linkage() function with the method='single' keyword argument. Assign the result to mergings.\n",
        "Plot a dendrogram of the hierarchical clustering, using the list country_names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you have done earlier."
      ],
      "metadata": {
        "id": "6nBCC34qZklJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Calculate the linkage: mergings\n",
        "mergings = linkage(samples, method='single')\n",
        "\n",
        "# Plot the dendrogram\n",
        "dendrogram(\n",
        "    mergings,\n",
        "    labels=country_names,\n",
        "    leaf_rotation=90,\n",
        "    leaf_font_size=6\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "by4Tu2RTZnzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting the cluster labels\n",
        "In the previous exercise, you saw that the intermediate clustering of the grain samples at height 6 has 3 clusters. Now, use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation.\n",
        "\n",
        "The hierarchical clustering has already been performed and mergings is the result of the linkage() function. The list varieties gives the variety of each grain sample.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "pandas as pd.\n",
        "fcluster from scipy.cluster.hierarchy.\n",
        "Perform a flat hierarchical clustering by using the fcluster() function on mergings. Specify a maximum height of 6 and the keyword argument criterion='distance'.\n",
        "Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.\n",
        "Create a cross-tabulation ct between df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label."
      ],
      "metadata": {
        "id": "H6XVBE4Cajg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "import pandas as pd\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Use fcluster to extract labels: labels\n",
        "labels = fcluster(mergings, t=6, criterion='distance')\n",
        "\n",
        "# Create a DataFrame with labels and varieties as columns: df\n",
        "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
        "\n",
        "# Create crosstab: ct\n",
        "ct = pd.crosstab(df['labels'], df['varieties'])\n",
        "\n",
        "# Display ct\n",
        "print(ct)\n"
      ],
      "metadata": {
        "id": "b7nvqR2wamEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# t-SNE visualization of grain dataset\n",
        "In the video, you saw t-SNE (t-distributed Stochastic Neighbor Embedding) applied to the iris dataset. In this exercise, you'll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. You are given an array samples of grain samples and a list variety_numbers giving the variety number of each grain sample.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import TSNE from sklearn.manifold.\n",
        "Create a TSNE instance called model with learning_rate=200.\n",
        "Apply the .fit_transform() method of model to samples. Assign the result to tsne_features.\n",
        "Select the column 0 of tsne_features. Assign the result to xs.\n",
        "Select the column 1 of tsne_features. Assign the result to ys.\n",
        "Make a scatter plot of the t-SNE features xs and ys. To color the points by the grain variety, specify the additional keyword argument c=variety_numbers."
      ],
      "metadata": {
        "id": "LqycV5jJGXAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=200)\n",
        "\n",
        "# Apply fit_transform to samples: tsne_features\n",
        "tsne_features = model.fit_transform(samples)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1st feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot, coloring by variety_numbers\n",
        "plt.scatter(xs, ys, c=variety_numbers)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W-pjUhviGf9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A t-SNE map of the stock market\n",
        "t-SNE provides great visualizations when the individual samples can be labeled. In this exercise, you'll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives you a map of the stock market! The stock price movements for each company are available as the array normalized_movements (these have already been normalized for you). The list companies gives the name of each company. PyPlot (plt) has been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import TSNE from sklearn.manifold.\n",
        "Create a TSNE instance called model with learning_rate=50.\n",
        "Apply the .fit_transform() method of model to normalized_movements. Assign the result to tsne_features.\n",
        "Select column 0 and column 1 of tsne_features.\n",
        "Make a scatter plot of the t-SNE features xs and ys. Specify the additional keyword argument alpha=0.5.\n",
        "Code to label each point with its company name has been written for you using plt.annotate(), so just hit submit to see the visualization!"
      ],
      "metadata": {
        "id": "3eLgMhHxHHM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TSNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Create a TSNE instance: model\n",
        "model = TSNE(learning_rate=50)\n",
        "\n",
        "# Apply fit_transform to normalized_movements: tsne_features\n",
        "tsne_features = model.fit_transform(normalized_movements)\n",
        "\n",
        "# Select the 0th feature: xs\n",
        "xs = tsne_features[:,0]\n",
        "\n",
        "# Select the 1th feature: ys\n",
        "ys = tsne_features[:,1]\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(xs, ys, alpha=0.5)\n",
        "\n",
        "# Annotate the points\n",
        "for x, y, company in zip(xs, ys, companies):\n",
        "    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "htRFKsBFHKdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlated data in nature\n",
        "You are given an array grains giving the width and length of samples of grain. You suspect that width and length will be correlated. To confirm this, make a scatter plot of width vs length and measure their Pearson correlation.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "matplotlib.pyplot as plt.\n",
        "pearsonr from scipy.stats.\n",
        "Assign column 0 of grains to width and column 1 of grains to length.\n",
        "Make a scatter plot with width on the x-axis and length on the y-axis.\n",
        "Use the pearsonr() function to calculate the Pearson correlation of width and length."
      ],
      "metadata": {
        "id": "mhh-cHgP3uhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assign the 0th column of grains: width\n",
        "width = grains[:,0]\n",
        "\n",
        "# Assign the 1st column of grains: length\n",
        "length = grains[:,1]\n",
        "\n",
        "# Scatter plot width vs length\n",
        "plt.scatter(width, length)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation\n",
        "correlation, pvalue = pearsonr(width, length)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "le1YHqZx3xXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decorrelating the grain measurements with PCA\n",
        "You observed in the previous exercise that the width and length measurements of the grain are correlated. Now, you'll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import PCA from sklearn.decomposition.\n",
        "Create an instance of PCA called model.\n",
        "Use the .fit_transform() method of model to apply the PCA transformation to grains. Assign the result to pca_features.\n",
        "The subsequent code to extract, plot, and compute the Pearson correlation of the first two columns pca_features has been written for you, so hit submit to see the result!"
      ],
      "metadata": {
        "id": "5Z2ff9DY4PVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Apply the fit_transform method of model to grains: pca_features\n",
        "pca_features = model.fit_transform(grains)\n",
        "\n",
        "# Assign 0th column of pca_features: xs\n",
        "xs = pca_features[:,0]\n",
        "\n",
        "# Assign 1st column of pca_features: ys\n",
        "ys = pca_features[:,1]\n",
        "\n",
        "# Scatter plot xs vs ys\n",
        "plt.scatter(xs, ys)\n",
        "plt.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "# Calculate the Pearson correlation of xs and ys\n",
        "correlation, pvalue = pearsonr(xs, ys)\n",
        "\n",
        "# Display the correlation\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "1oL0_jqF4SDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The first principal component\n",
        "The first principal component of the data is the direction in which the data varies the most. In this exercise, your job is to use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot.\n",
        "\n",
        "The array grains gives the length and width of the grain samples. PyPlot (plt) and PCA have already been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Make a scatter plot of the grain measurements. This has been done for you.\n",
        "Create a PCA instance called model.\n",
        "Fit the model to the grains data.\n",
        "Extract the coordinates of the mean of the data using the .mean_ attribute of model.\n",
        "Get the first principal component of model using the .components_[0,:] attribute.\n",
        "Plot the first principal component as an arrow on the scatter plot, using the plt.arrow() function. You have to specify the first two arguments - mean[0] and mean[1]."
      ],
      "metadata": {
        "id": "8ePoTPTpBoHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a scatter plot of the untransformed points\n",
        "plt.scatter(grains[:,0], grains[:,1])\n",
        "\n",
        "# Create a PCA instance: model\n",
        "model = PCA()\n",
        "\n",
        "# Fit model to points\n",
        "model.fit(grains)\n",
        "\n",
        "# Get the mean of the grain samples: mean\n",
        "mean = model.mean_\n",
        "\n",
        "# Get the first principal component: first_pc\n",
        "first_pc = model.components_[0,:]\n",
        "\n",
        "# Plot first_pc as an arrow, starting at mean\n",
        "plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n",
        "\n",
        "# Keep axes on same scale\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JPp-1IHpBqkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance of the PCA features\n",
        "The fish dataset is 6-dimensional. But what is its intrinsic dimension? Make a plot of the variances of the PCA features to find out. As before, samples is a 2D array, where each row represents a fish. You'll need to standardize the features first.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an instance of StandardScaler called scaler.\n",
        "Create a PCA instance called pca.\n",
        "Use the make_pipeline() function to create a pipeline chaining scaler and pca.\n",
        "Use the .fit() method of pipeline to fit it to the fish samples samples.\n",
        "Extract the number of components used using the .n_components_ attribute of pca. Place this inside a range() function and store the result as features.\n",
        "Use the plt.bar() function to plot the explained variances, with features on the x-axis and pca.explained_variance_ on the y-axis."
      ],
      "metadata": {
        "id": "pPYAiiFuCOXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the necessary imports\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scaler: scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Create a PCA instance: pca\n",
        "pca = PCA()\n",
        "\n",
        "# Create pipeline: pipeline\n",
        "pipeline = make_pipeline(scaler, pca)\n",
        "\n",
        "# Fit the pipeline to 'samples'\n",
        "pipeline.fit(samples)\n",
        "\n",
        "# Plot the explained variances\n",
        "features = range(pca.n_components_)\n",
        "plt.bar(features, pca.explained_variance_)\n",
        "plt.xlabel('PCA feature')\n",
        "plt.ylabel('variance')\n",
        "plt.xticks(features)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "moblLzvGCQwV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsr8wYL3AAI03LJM2SOg4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_4_IntermediateDeepLearningWithPyTorch_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is the start for Module 4."
      ],
      "metadata": {
        "id": "d2G-39ot8LIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Dataset\n",
        "Time to refresh your PyTorch Datasets knowledge!\n",
        "\n",
        "Before model training can commence, you need to load the data and pass it to the model in the right format. In PyTorch, this is handled by Datasets and DataLoaders. Let's start with building a PyTorch Dataset for our water potability data.\n",
        "\n",
        "In this exercise, you will define a class called WaterDataset to load the data from a CSV file. To do this, you will need to implement the three methods which PyTorch expects a Dataset to have:\n",
        "\n",
        ".__init__() to load the data,\n",
        ".__len__() to return data size,\n",
        ".__getitem()__ to extract features and label for a single sample.\n",
        "The following imports that you need have already been done for you:\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "Instructions 1/3\n",
        "In the .__init__() method, load the data from csv_path to a pandas DataFrame and assign it to df.\n",
        "Convert df to a NumPy array and assign the result to self.data."
      ],
      "metadata": {
        "id": "g3oOswAeTVfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaterDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        super().__init__()\n",
        "        # Load data to pandas DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # Convert data to a NumPy array and assign to self.data\n",
        "        self.data = df.to_numpy()"
      ],
      "metadata": {
        "id": "tiWfcW_xTZjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "Implement the .__len__() method to return the number of data samples."
      ],
      "metadata": {
        "id": "b_fmVkLlUBZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaterDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        super().__init__()\n",
        "        # Load data to pandas DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # Convert data to a NumPy array and assign to self.data\n",
        "        self.data = df.to_numpy()\n",
        "\n",
        "    # Implement __len__ to return the number of data samples\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "metadata": {
        "id": "bjX4mwv5UCYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "In the .__getitem__() method, get the label by slicing self.data to extract its last column for the index idx, similarly to how it's done for the features."
      ],
      "metadata": {
        "id": "apm3KEzDUS7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaterDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        super().__init__()\n",
        "        # Load data to pandas DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # Convert data to a NumPy array and assign to self.data\n",
        "        self.data = df.to_numpy()\n",
        "\n",
        "    # Implement __len__ to return the number of data samples\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = self.data[idx, :-1]\n",
        "        # Assign last data column to label\n",
        "        label = self.data[idx, -1]\n",
        "        return features, label"
      ],
      "metadata": {
        "id": "iAF7GLJ4UTd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch DataLoader\n",
        "Good job defining the Dataset class! The WaterDataset you just created is now available for you to use.\n",
        "\n",
        "The next step in preparing the training data is to set up a DataLoader. A PyTorch DataLoader can be created from a Dataset to load data, split it into batches, and perform transformations on the data if desired. Then, it yields a data sample ready for training.\n",
        "\n",
        "In this exercise, you will build a DataLoader based on the WaterDataset. The DataLoader class you will need has already been imported for you from torch.utils.data. Let's get to it!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an instance of WaterDataset from water_train.csv, assigning it to dataset_train.\n",
        "Create dataloader_train based on dataset_train, using a batch size of two and shuffling the samples.\n",
        "Get a batch of features and labels from the DataLoader and print them."
      ],
      "metadata": {
        "id": "F9Y8ikstVAxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the WaterDataset\n",
        "dataset_train = WaterDataset(\"water_train.csv\")\n",
        "\n",
        "# Create a DataLoader based on dataset_train\n",
        "dataloader_train = DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# Get a batch of features and labels\n",
        "features, labels = next(iter(dataloader_train))\n",
        "print(features, labels)"
      ],
      "metadata": {
        "id": "X15i2qHkVC7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Model\n",
        "You will use the OOP approach to define the model architecture. Recall that this requires setting up a model class and defining two methods inside it:\n",
        "\n",
        ".__init__(), in which you define the layers you want to use;\n",
        "\n",
        "forward(), in which you define what happens to the model inputs once it receives them; this is where you pass inputs through pre-defined layers.\n",
        "\n",
        "Let's build a model with three linear layers and ReLU activations. After the last linear layer, you need a sigmoid activation instead, which is well-suited for binary classification tasks like our water potability prediction problem. Here's the model defined using nn.Sequential(), which you may be more familiar with:\n",
        "\n",
        "net = nn.Sequential(\n",
        "  nn.Linear(9, 16),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(16, 8),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(8, 1),\n",
        "  nn.Sigmoid(),\n",
        ")\n",
        "Let's rewrite this model as a class!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "In the .__init__() method, define the three linear layers with dimensions corresponding to the model definition provided and assign them to self.fc1, self.fc2, and self.fc3, respectively.\n",
        "In the forward() method, pass the model input x through all the layers, remembering to add activations on top of them, similarly how it's already done for the first layer."
      ],
      "metadata": {
        "id": "WYdImEqnVpkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the three linear layers\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass x through linear layers adding activations\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = nn.functional.relu(self.fc2(x))\n",
        "        x = nn.functional.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "MEWvHCK-Vtlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers\n",
        "It's time to explore the different optimizers that you can use for training your model.\n",
        "\n",
        "A custom function called train_model(optimizer, net, num_epochs) has been defined for you. It takes the optimizer, the model, and the number of epochs as inputs, runs the training loops, and prints the training loss at the end.\n",
        "\n",
        "Let's use train_model() to run a few short trainings with different optimizers and compare the results!\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Define the optimizer as Stochastic Gradient Descent."
      ],
      "metadata": {
        "id": "nZMdqHm1i5F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Define the SGD optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "train_model(\n",
        "    optimizer=optimizer,\n",
        "    net=net,\n",
        "    num_epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "sHcXFF2Li8Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "Define the optimizer as Root Mean Square Propagation (RMSprop), passing the model's parameters as its first argument."
      ],
      "metadata": {
        "id": "9O_TCpa7jD8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Define the RMSprop optimizer\n",
        "optimizer = optim.RMSprop(net.parameters(), lr=0.001)\n",
        "\n",
        "train_model(\n",
        "    optimizer=optimizer,\n",
        "    net=net,\n",
        "    num_epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "aLjmiCetjFkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "Define the optimizer as Adaptive Moments Estimation (Adam), setting the learning rate to 0.001."
      ],
      "metadata": {
        "id": "gAKfWVSpjRX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# Define the Adam optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "train_model(\n",
        "    optimizer=optimizer,\n",
        "    net=net,\n",
        "    num_epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "7w2CkEd0jUV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation\n",
        "With the training loop sorted out, you have trained the model for 1000 epochs, and it is available to you as net. You have also set up a test_dataloader in exactly the same way as you did with train_dataloader before—just reading the data from the test rather than the train directory.\n",
        "\n",
        "You can now evaluate the model on test data. To do this, you will need to write the evaluation loop to iterate over the batches of test data, get the model's predictions for each batch, and calculate the accuracy score for it. Let's do it!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set up the evaluation metric as Accuracy for binary classification and assign it to acc.\n",
        "For each batch of test data, get the model's outputs and assign them to outputs.\n",
        "After the loop, compute the total test accuracy and assign it to test_accuracy."
      ],
      "metadata": {
        "id": "lygni7pSjwRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Set up binary accuracy metric\n",
        "acc = Accuracy(task=\"binary\")\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for features, labels in dataloader_test:\n",
        "        # Get predicted probabilities for test data batch\n",
        "        outputs = net(features)\n",
        "        preds = (outputs >= 0.5).float()\n",
        "        acc(preds, labels.view(-1, 1))\n",
        "\n",
        "# Compute total test accuracy\n",
        "test_accuracy = acc.compute()\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "NyAaSUNcj1BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization and activation\n",
        "The problems of unstable (vanishing or exploding) gradients are a challenge that often arises in training deep neural networks. In this and the following exercises, you will expand the model architecture that you built for the water potability classification task to make it more immune to those problems.\n",
        "\n",
        "As a first step, you'll improve the weights initialization by using He (Kaiming) initialization strategy. To do so, you will need to call the proper initializer from the torch.nn.init module, which has been imported for you as init. Next, you will update the activations functions from the default ReLU to the often better ELU.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Call the He (Kaiming) initializer on the weight attribute of the second layer, fc2, similarly to how it's done for fc1.\n",
        "Call the He (Kaiming) initializer on the weight attribute of the third layer, fc3, accounting for the different activation function used in the final layer.\n",
        "Update the activation functions in the forward() method from relu to elu."
      ],
      "metadata": {
        "id": "Kq0qRUNdrTx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        # Apply He initialization\n",
        "        init.kaiming_uniform_(self.fc1.weight)\n",
        "        init.kaiming_uniform_(self.fc2.weight)\n",
        "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Update ReLU activation to ELU\n",
        "        x = nn.functional.elu(self.fc1(x))\n",
        "        x = nn.functional.elu(self.fc2(x))\n",
        "        x = nn.functional.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "b_RNRTcQ7B58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Normalization\n",
        "As a final improvement to the model architecture, let's add the batch normalization layer after each of the two linear layers. The batch norm trick tends to accelerate training convergence and protects the model from vanishing and exploding gradients issues.\n",
        "\n",
        "Both torch.nn and torch.nn.init have already been imported for you as nn and init, respectively. Once you implement the change in the model architecture, be ready to answer a short question on how batch normalization works!\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Add two BatchNorm1d layers assigning them to self.bn1 and self.bn2."
      ],
      "metadata": {
        "id": "tlmI6oZR7tZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        # Add two batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.bn2 = nn.BatchNorm1d(8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        init.kaiming_uniform_(self.fc1.weight)\n",
        "        init.kaiming_uniform_(self.fc2.weight)\n",
        "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")"
      ],
      "metadata": {
        "id": "DbtP5YN37vr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "In the forward() method, pass x through the second set of layers: the linear layer, the batch norm layer, and the activations, similarly to how it's done for the first set of layers."
      ],
      "metadata": {
        "id": "tIV7AZlk7yjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(9, 16)\n",
        "        # Add two batch normalization layers\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.bn2 = nn.BatchNorm1d(8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        init.kaiming_uniform_(self.fc1.weight)\n",
        "        init.kaiming_uniform_(self.fc2.weight)\n",
        "        init.kaiming_uniform_(self.fc3.weight, nonlinearity=\"sigmoid\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = nn.functional.elu(x)\n",
        "\n",
        "        # --- Code Added ---\n",
        "        # Pass x through the second set of layers\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = nn.functional.elu(x)\n",
        "\n",
        "        x = nn.functional.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Part 3/3 of Batch Normalization is just a question. No code."
      ],
      "metadata": {
        "id": "RoDI706w7zjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image dataset\n",
        "Let's start with building a Torch Dataset of images. You'll use it to explore the data and, later, to feed it into a model.\n",
        "\n",
        "The training data for the cloud classification task is stored in the following directory structure:\n",
        "\n",
        "\n",
        "```\n",
        "clouds_train\n",
        "  - cirriform clouds\n",
        "    - 539cd1c356e9c14749988a12fdf6c515.jpg\n",
        "    - ...\n",
        "  - clear sky\n",
        "  - cumulonimbus clouds\n",
        "  - cumulus clouds\n",
        "  - high cumuliform clouds\n",
        "  - stratiform clouds\n",
        "  - stratocumulus clouds\n",
        "\n",
        "```\n",
        "\n",
        "There are seven folders inside clouds_train, each representing one cloud type (or a clear sky). Inside each of these folders sit corresponding image files.\n",
        "\n",
        "The following imports have already been done for you:\n",
        "\n",
        "\n",
        "```\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "```\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Compose two transformations, the first, to parse the image to a tensor, and one to resize the image to 128 by 128, assigning them to train_transforms.\n",
        "Use ImageFolder to define dataset_train, passing it the directory path to the data (\"clouds_train\") and the transforms defined earlier.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hlCwQUiHlnJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compose transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((128, 128)),\n",
        "])\n",
        "\n",
        "# Create Dataset using ImageFolder\n",
        "dataset_train = ImageFolder(\n",
        "    \"clouds_train\",\n",
        "    transform=train_transforms,\n",
        ")"
      ],
      "metadata": {
        "id": "mzaVqg2sl4DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation in PyTorch\n",
        "Let's include data augmentation in your Dataset and inspect some images visually to make sure the desired transformations are applied.\n",
        "\n",
        "First, you'll add the augmenting transformations to train_transforms. Let's use a random horizontal flip and a rotation by a random angle between 0 and 45 degrees. The code that follows to create the Dataset and the DataLoader is exactly the same as before. Finally, you'll reshape the image and display it to see if the new augmenting transformations are visible.\n",
        "\n",
        "All the imports you need have been called for you:\n",
        "\n",
        "\n",
        "```\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "Time to augment some cloud photos!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add two more transformations to train_transforms to perform a random horizontal flip and then a rotation by a random angle between 0 and 45 degrees.\n",
        "Reshape the image tensor from the DataLoader to make it suitable for display.\n",
        "Display the image.\n"
      ],
      "metadata": {
        "id": "UAgEWzoFmq8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    # Add horizontal flip and rotation\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((128, 128)),\n",
        "])\n",
        "\n",
        "dataset_train = ImageFolder(\n",
        "  \"clouds_train\",\n",
        "  transform=train_transforms,\n",
        ")\n",
        "\n",
        "dataloader_train = DataLoader(\n",
        "  dataset_train, shuffle=True, batch_size=1\n",
        ")\n",
        "\n",
        "image, label = next(iter(dataloader_train))\n",
        "# Reshape the image tensor\n",
        "image = image.squeeze().permute(1, 2, 0)\n",
        "# Display the image\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WjtviupHmwql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building convolutional networks\n",
        "You are on a team building a weather forecasting system. As part of the system, cameras will be installed at various locations to take pictures of the sky. Your task is to build a model to classify different cloud types in these pictures, which will help spot approaching weather fronts.\n",
        "\n",
        "You decide to build a convolutional image classifier. The model will consist of two parts:\n",
        "\n",
        "A feature extractor that learns a vector of features from the input image,\n",
        "A classifier that predicts the image's class based on the learned features.\n",
        "Both torch and torch.nn as nn have already been imported for you, so let's get to it!\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Define the feature_extractor part of the model by adding another convolutional layer with 64 output feature maps, the ELU activation, and a max pooling layer with a window of size two; at the end, flatten the output."
      ],
      "metadata": {
        "id": "6sJwg8Fu4QDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Define feature extractor\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Flatten(),\n",
        "        )"
      ],
      "metadata": {
        "id": "WDL3bhJH4TFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "Define the classifier part of the model as a single linear layer with a number of inputs that reflects an input image of 64x64 and the feature extractor defined; the classifier should have num_classes outputs."
      ],
      "metadata": {
        "id": "BaDP8m9b4fCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Define feature extractor\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        # Define classifier\n",
        "        self.classifier = nn.Linear(64*16*16, num_classes)"
      ],
      "metadata": {
        "id": "UeOLFam74gg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "In the forward() method, pass the input image x first through the feature extractor and then through the classifier."
      ],
      "metadata": {
        "id": "saDTpt_q4zyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Define feature extractor\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ELU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        # Define classifier\n",
        "        self.classifier = nn.Linear(64*16*16, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through feature extractor and classifier\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EhCBrptW40EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset with augmentations\n",
        "You have already built the image dataset from cloud pictures and the convolutional model to classify different cloud types. Before you train it, let's adapt the dataset by adding the augmentations that could improve the model's cloud classification performance.\n",
        "\n",
        "The code to set up the Dataset and DataLoader is already prepared for you and should look familiar. Your task is to define the composition of transforms that will be applied to the input images as they are loaded.\n",
        "\n",
        "Note that before you were resizing images to 128 by 128 to display them nicely, but now you will use smaller ones to speed up training. As you will see later, 64 by 64 will be large enough for the model to learn.\n",
        "\n",
        "from torchvision import transforms has been already executed for you, so let's get to it!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define train_transforms by composing together five transformations: a random horizontal flip, random rotation (by angle from 0 to 45 degrees), random automatic contrast adjustment, parsing to tensor, and resizing to 64 by 64 pixels."
      ],
      "metadata": {
        "id": "8RfI9nf-FNjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(45),\n",
        "    transforms.RandomAutocontrast(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((64, 64)),\n",
        "])\n",
        "\n",
        "dataset_train = ImageFolder(\n",
        "  \"clouds_train\",\n",
        "  transform=train_transforms,\n",
        ")\n",
        "dataloader_train = DataLoader(\n",
        "  dataset_train, shuffle=True, batch_size=16\n",
        ")"
      ],
      "metadata": {
        "id": "mGrtfbedFPzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classifier training loop\n",
        "It's time to train the image classifier! You will use the Net you defined earlier and train it to distinguish between seven cloud types.\n",
        "\n",
        "To define the loss and optimizer, you will need to use functions from torch.nn and torch.optim, imported for you as nn and optim, respectively. You don't need to change anything in the training loop itself: it's exactly like the ones you wrote before, with some additional logic to print the loss during training.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define the model using your Net class with num_classes set to 7 and assign it to net.\n",
        "Define the loss function as cross-entropy loss and assign it to criterion.\n",
        "Define the optimizer as Adam, passing it the model's parameters and the learning rate of 0.001, and assign it to optimizer.\n",
        "Start the training for-loop by iterating over training images and labels of dataloader_train."
      ],
      "metadata": {
        "id": "Xe-mknPYGAxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "net = Net(num_classes=7)\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(3):\n",
        "    running_loss = 0.0\n",
        "    # Iterate over training batches\n",
        "    for images, labels in dataloader_train:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader_train)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "AC3TtR8eGBcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class model evaluation\n",
        "Let's evaluate our cloud classifier with precision and recall to see how well it can classify the seven cloud types. In this multi-class classification task it is important how you average the scores over classes. Recall that there are four approaches:\n",
        "\n",
        "Not averaging, and analyzing the results per class;\n",
        "Micro-averaging, ignoring the classes and computing the metrics globally;\n",
        "Macro-averaging, computing metrics per class and averaging them;\n",
        "Weighted-averaging, just like macro but with the average weighted by class size.\n",
        "Both Precision and Recall are already imported from torchmetrics. It's time to see how well our model is doing!\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Define precision and recall metrics calculated globally on all examples."
      ],
      "metadata": {
        "id": "0FvNcm525W93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics\n",
        "metric_precision = Precision(task=\"multiclass\", num_classes=7, average=\"micro\")\n",
        "metric_recall = Recall(task=\"multiclass\", num_classes=7, average=\"micro\")\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataloader_test:\n",
        "        outputs = net(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        metric_precision(preds, labels)\n",
        "        metric_recall(preds, labels)\n",
        "\n",
        "precision = metric_precision.compute()\n",
        "recall = metric_recall.compute()\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")"
      ],
      "metadata": {
        "id": "kHLZYJI75Zuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Change your code to compute separate recall and precision metrics for each class and average them with a simple average."
      ],
      "metadata": {
        "id": "gRTRsw1k5v7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics\n",
        "metric_precision = Precision(task=\"multiclass\", num_classes=7, average=\"macro\")\n",
        "metric_recall = Recall(task=\"multiclass\", num_classes=7, average=\"macro\")\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataloader_test:\n",
        "        outputs = net(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        metric_precision(preds, labels)\n",
        "        metric_recall(preds, labels)\n",
        "\n",
        "precision = metric_precision.compute()\n",
        "recall = metric_recall.compute()\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")"
      ],
      "metadata": {
        "id": "MdMA6SdU5-xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing metrics per class\n",
        "While aggregated metrics are useful indicators of the model's performance, it is often informative to look at the metrics per class. This could reveal classes for which the model underperforms.\n",
        "\n",
        "In this exercise, you will run the evaluation loop again to get our cloud classifier's precision, but this time per-class. Then, you will map these score to the class names to interpret them. As usual, Precision has already been imported for you. Good luck!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a precision metric appropriate for per-class results.\n",
        "Calculate the precision per class by finishing the dict comprehension, iterating over the .items() of the .class_to_idx attribute of dataset_test."
      ],
      "metadata": {
        "id": "DcDPqNd26JZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define precision metric\n",
        "metric_precision = Precision(\n",
        "    task=\"multiclass\", num_classes=7, average=None\n",
        ")\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataloader_test:\n",
        "        outputs = net(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        metric_precision(preds, labels)\n",
        "precision = metric_precision.compute()\n",
        "\n",
        "# Get precision per class\n",
        "precision_per_class = {\n",
        "    k: precision[v].item()\n",
        "    for k, v\n",
        "    in dataset_test.class_to_idx.items()\n",
        "}\n",
        "print(precision_per_class)"
      ],
      "metadata": {
        "id": "VkzBEVKn6LBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating sequences\n",
        "To be able to train neural networks on sequential data, you need to pre-process it first. You'll chunk the data into inputs-target pairs, where the inputs are some number of consecutive data points and the target is the next data point.\n",
        "\n",
        "Your task is to define a function to do this called create_sequences(). As inputs, it will receive data stored in a DataFrame, df and seq_length, the length of the inputs. As outputs, it should return two NumPy arrays, one with input sequences and the other one with the corresponding targets.\n",
        "\n",
        "As a reminder, here is how the DataFrame df looks like:\n",
        "\n",
        "```\n",
        "                 timestamp  consumption\n",
        "0      2011-01-01 00:15:00    -0.704319\n",
        "...                    ...          ...\n",
        "140255 2015-01-01 00:00:00    -0.095751\n",
        "```\n",
        "Instructions\n",
        "100 XP\n",
        "Iterate over the range of the number of data points minus the length of an input sequence.\n",
        "Define the inputs x as the slice of df from the ith row to the i + seq_lengthth row and the column at index 1.\n",
        "Define the target y as the slice of df at row index i + seq_length and the column at index 1."
      ],
      "metadata": {
        "id": "05je-xCKiJZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(df, seq_length):\n",
        "    xs, ys = [], []\n",
        "    # Iterate over data indices\n",
        "    for i in range(len(df) - seq_length):\n",
        "      \t# Define inputs\n",
        "        x = df.iloc[i:(i+seq_length), 1]\n",
        "        # Define target\n",
        "        y = df.iloc[i+seq_length, 1]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)"
      ],
      "metadata": {
        "id": "Y6PTaFwTiOFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequential Dataset\n",
        "Good job building the create_sequences() function! It's time to use it to create a training dataset for your model.\n",
        "\n",
        "Just like tabular and image data, sequential data is easiest passed to a model through a torch Dataset and DataLoader. To build a sequential Dataset, you will call create_sequences() to get the NumPy arrays with inputs and targets, and inspect their shape. Next, you will pass them to a TensorDataset to create a proper torch Dataset, and inspect its length.\n",
        "\n",
        "Your implementation of create_sequences() and a DataFrame with the training data called train_data are available.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Call create_sequences(), passing it the training DataFrame and a sequence length of 24*4, assigning the result to X_train, y_train.\n",
        "Define dataset_train by calling TensorDataset and passing it two arguments, the inputs and the targets created by create_sequences(), both converted from NumPy arrays to tensors of floats."
      ],
      "metadata": {
        "id": "GW1w05q3iXA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Use create_sequences to create inputs and targets\n",
        "X_train, y_train = create_sequences(train_data, 24*4)\n",
        "print(X_train.shape, y_train.shape)\n",
        "\n",
        "# Create TensorDataset\n",
        "dataset_train = TensorDataset(\n",
        "    torch.from_numpy(X_train).float(),\n",
        "    torch.from_numpy(y_train).float(),\n",
        ")\n",
        "print(len(dataset_train))"
      ],
      "metadata": {
        "id": "UVapXGXkiXpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a forecasting RNN\n",
        "It's time to build your first recurrent network! It will be a sequence-to-vector model consisting of an RNN layer with two layers and a hidden_size of 32. After the RNN layer, a simple linear layer will map the outputs to a single value to be predicted.\n",
        "\n",
        "The following imports have already been done for you:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "Define the RNN layer passing it the correct values for input_size, hidden_size, num_layers, and batch_first, and assign it to self.rnn."
      ],
      "metadata": {
        "id": "cMYMF6OdOtL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)"
      ],
      "metadata": {
        "id": "T8mRGqSwOvFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "Initialize the first hidden state h0 as a tensor of zeros of the appropriate shape."
      ],
      "metadata": {
        "id": "__3aeOahO6Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    #------ Added Code -------\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize first hidden state with zeros\n",
        "        h0 = torch.zeros(2, x.size(0), 32)"
      ],
      "metadata": {
        "id": "yEyXmRDUO71Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "Pass the input x and the first hidden state h0 through recurrent layer."
      ],
      "metadata": {
        "id": "lVQ49c7LPUZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize first hidden state with zeros\n",
        "        h0 = torch.zeros(2, x.size(0), 32)\n",
        "\n",
        "        #------ Added Code ------\n",
        "        # Pass x and h0 through recurrent layer\n",
        "        out, _ = self.rnn(x, h0)"
      ],
      "metadata": {
        "id": "HFBCPU6jPVpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "Pass recurrent layer's last output through the linear layer"
      ],
      "metadata": {
        "id": "Rup-YjxhPtIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define RNN layer\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize first hidden state with zeros\n",
        "        h0 = torch.zeros(2, x.size(0), 32)\n",
        "        # Pass x and h0 through recurrent layer\n",
        "        out, _ = self.rnn(x, h0)\n",
        "\n",
        "        #------ Code Added ------\n",
        "        # Pass recurrent layer's last output through linear layer\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "jU46Xdu1PtwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM network\n",
        "As you already know, plain RNN cells are not used that much in practice. A more frequently used alternative that ensures a much better handling of long sequences are Long Short-Term Memory cells, or LSTMs. In this exercise, you will be build an LSTM network yourself!\n",
        "\n",
        "The most important implementation difference from the RNN network you have built previously comes from the fact that LSTMs have two rather than one hidden states. This means you will need to initialize this additional hidden state and pass it to the LSTM cell.\n",
        "\n",
        "torch and torch.nn have already been imported for you, so start coding!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "In the .__init__() method, define an LSTM layer and assign it to self.lstm.\n",
        "In the forward() method, initialize the first long-term memory hidden state c0 with zeros.\n",
        "In the forward() method, pass all three inputs to the LSTM layer: the current time step's inputs, and a tuple containing the two hidden states."
      ],
      "metadata": {
        "id": "LvjRXJ9TaMwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        # Define lstm layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(2, x.size(0), 32)\n",
        "        # Initialize long-term memory\n",
        "        c0 = torch.zeros(2, x.size(0), 32)\n",
        "        # Pass all inputs to lstm layer\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "QRupv8MXaNRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU network\n",
        "Next to LSTMs, another popular recurrent neural network variant is the Gated Recurrent Unit, or GRU. It's appeal is in its simplicity: GRU cells require less computation than LSTM cells while often matching them in performance.\n",
        "\n",
        "The code you are provided with is the RNN model definition that you coded previously. Your task is to adapt it such that it produces a GRU network instead. torch and torch.nn as nn have already been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Update the RNN model definition in order to obtain a GRU network; assign the GRU layer to self.gru."
      ],
      "metadata": {
        "id": "H2CT3F2QQsQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define RNN layer\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=1,\n",
        "            hidden_size=32,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(2, x.size(0), 32)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "nTgpGaQSQsjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN training loop\n",
        "It's time to train the electricity consumption forecasting model!\n",
        "\n",
        "You will use the LSTM network you have defined previously, which has been instantiated and assigned to net, as is the dataloader_train you built before. You will also need to use torch.nn which has already been imported as nn.\n",
        "\n",
        "In this exercise, you will train the model for only three epochs to make sure the training progresses as expected. Let's get to it!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set up the Mean Squared Error loss and assign it to criterion.\n",
        "Reshape seqs to (batch size, sequence length, num features), which in our case is (32, 96, 1), and re-assign the result to seqs.\n",
        "Pass seqs to the model to get its outputs.\n",
        "Based on previously computed quantities, calculate the loss, assigning it to loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "xiIfzcZmcXdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "# Set up MSE loss\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(\n",
        "  net.parameters(), lr=0.0001\n",
        ")\n",
        "\n",
        "for epoch in range(3):\n",
        "    for seqs, labels in dataloader_train:\n",
        "        # Reshape model inputs\n",
        "        seqs = seqs.view(32, 96, 1)\n",
        "        # Get model outputs\n",
        "        outputs = net(seqs)\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "lY5aOvw1cXwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating forecasting models\n",
        "It's evaluation time! The same LSTM network that you have trained in the previous exercise has been trained for you for a few more epochs and is available as net.\n",
        "\n",
        "Your task is to evaluate it on a test dataset using the Mean Squared Error metric (torchmetrics has already been imported for you). Let's see how well the model is doing!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define the Mean Squared Error metrics and assign it to mse.\n",
        "Pass the input sequence to net, and squeeze the result before you assign it to outputs.\n",
        "Compute the final value of the test metric assigning it to test_mse."
      ],
      "metadata": {
        "id": "vnih4HlRdEoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define MSE metric\n",
        "mse = torchmetrics.MeanSquaredError()\n",
        "\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    for seqs, labels in dataloader_test:\n",
        "        seqs = seqs.view(32, 96, 1)\n",
        "        # Pass seqs to net and squeeze the result\n",
        "        outputs = net(seqs).squeeze()\n",
        "        mse(outputs, labels)\n",
        "\n",
        "# Compute final metric value\n",
        "test_mse = mse.compute()\n",
        "print(f\"Test MSE: {test_mse}\")"
      ],
      "metadata": {
        "id": "-5wBCknBdF-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two-input dataset\n",
        "Building a multi-input model starts with crafting a custom dataset that can supply all the inputs to the model. In this exercise, you will build the Omniglot dataset that serves triplets consisting of:\n",
        "\n",
        "The image of a character to be classified,\n",
        "The one-hot encoded alphabet vector of length 30, with zeros everywhere but for a single one denoting the ID of the alphabet the character comes from,\n",
        "The target label, an integer between 0 and 963.\n",
        "You are provided with samples, a list of 3-tuples comprising an image's file path, its alphabet vector, and the target label. Also, the following imports have already been done for you, so let's get to it!\n",
        "\n",
        "```\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "```\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "Assign transform and samples to class attributes with the same names."
      ],
      "metadata": {
        "id": "jnmBtZ5IOBUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OmniglotDataset(Dataset):\n",
        "    def __init__(self, transform, samples):\n",
        "\t\t# Assign transform and samples to class attributes\n",
        "        self.transform = transform\n",
        "        self.samples = samples"
      ],
      "metadata": {
        "id": "A5QnAzRWOghW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "\n",
        "Implement the .__len()__ method such that it returns the number of samples stored in the class' samples attribute."
      ],
      "metadata": {
        "id": "ttmnMnhZOkJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OmniglotDataset(Dataset):\n",
        "    def __init__(self, transform, samples):\n",
        "\t\t# Assign transform and samples to class attributes\n",
        "        self.transform = transform\n",
        "        self.samples = samples\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return number of samples\n",
        "        return len(self.samples)"
      ],
      "metadata": {
        "id": "rXu2DYPYOmTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gyg5gHw5O29G"
      }
    }
  ]
}
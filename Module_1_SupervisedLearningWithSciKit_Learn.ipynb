{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnT9/ibglIC26LccWyaIJK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_1_SupervisedLearningWithSciKit_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Nearest Neighbor Classifier\n",
        "In this exercise, you will build your first classification model using the churn_df dataset, which has been preloaded for the remainder of the chapter.\n",
        "\n",
        "The target, \"churn\", needs to be a single column with the same number of observations as the feature data. The feature data has already been converted into numpy arrays.\n",
        "\n",
        "\"account_length\" and \"customer_service_calls\" are treated as features because account length indicates customer loyalty, and frequent customer service calls may signal dissatisfaction, both of which can be good predictors of churn.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import KNeighborsClassifier from sklearn.neighbors.\n",
        "Instantiate a KNeighborsClassifier called knn with 6 neighbors.\n",
        "Fit the classifier to the data using the .fit() method."
      ],
      "metadata": {
        "id": "gMwcSsY-yn-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "y = churn_df[\"churn\"].values\n",
        "X = churn_df[[\"account_length\", \"customer_service_calls\"]].values\n",
        "\n",
        "# Create a KNN classifier with 6 neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "# Fit the classifier to the data\n",
        "knn.fit(X, y)"
      ],
      "metadata": {
        "id": "jYxPcuIGxbzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting using K-Nearest Neighbor (KNN)\n",
        "Now you have fit a KNN classifier, you can use it to predict the label of new data points. All available data was used for training, however, fortunately, there are new observations available. These have been preloaded for you as X_new.\n",
        "\n",
        "The model knn, which you created and fit the data in the last exercise, has been preloaded for you. You will use your classifier to predict the labels of a set of new data points:\n",
        "\n",
        "X_new = np.array([[30.0, 17.5],\n",
        "                  [107.0, 24.1],\n",
        "                  [213.0, 10.9]])\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create y_pred by predicting the target values of the unseen features X_new using the knn model.\n",
        "Print the predicted labels for the set of predictions."
      ],
      "metadata": {
        "id": "7rqCxl1XzW5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the labels for the X_new\n",
        "y_pred = knn.predict(X_new)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Predictions: {}\".format(y_pred))"
      ],
      "metadata": {
        "id": "7Ph-GV2ozbpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/test split + computing accuracy\n",
        "It's time to practice splitting your data into training and test sets with the churn_df dataset!\n",
        "\n",
        "NumPy arrays have been created for you containing the features as X and the target variable as y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import train_test_split from sklearn.model_selection.\n",
        "Split X and y into training and test sets, setting test_size equal to 20%, random_state to 42, and ensuring the target label proportions reflect that of the original dataset.\n",
        "Fit the knn model to the training data.\n",
        "Compute and print the model's accuracy for the test data."
      ],
      "metadata": {
        "id": "myPyx4xhz1FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the module\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = churn_df.drop(\"churn\", axis=1).values\n",
        "y = churn_df[\"churn\"].values\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Print the accuracy\n",
        "print(knn.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "8TpxXfNpz7NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting and underfitting\n",
        "Interpreting model complexity is a great way to evaluate supervised learning performance. Your aim is to produce a model that can interpret the relationship between features and the target variable, as well as generalize well when exposed to new observations.\n",
        "\n",
        "The training and test sets have been created from the churn_df dataset and preloaded as X_train, X_test, y_train, and y_test.\n",
        "\n",
        "In addition, KNeighborsClassifier has been imported for you along with numpy as np.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create neighbors as a numpy array of values from 1 up to and including 12.\n",
        "Instantiate a KNeighborsClassifier, with the number of neighbors equal to the neighbor iterator.\n",
        "Fit the model to the training data.\n",
        "Calculate accuracy scores for the training set and test set separately using the .score() method, and assign the results to the train_accuracies and test_accuracies dictionaries, respectively, utilizing the neighbor iterator as the index."
      ],
      "metadata": {
        "id": "BuYsC0Hlz84b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create neighbors\n",
        "neighbors = np.arange(1, 13)\n",
        "train_accuracies = {}\n",
        "test_accuracies = {}\n",
        "\n",
        "for neighbor in neighbors:\n",
        "\t# Set up a KNN Classifier\n",
        "\tknn = KNeighborsClassifier(n_neighbors=neighbor)\n",
        "\t# Fit the model\n",
        "\tknn.fit(X_train, y_train)\n",
        "\t# Compute accuracy\n",
        "\ttrain_accuracies[neighbor] = knn.score(X_train, y_train)\n",
        "\ttest_accuracies[neighbor] = knn.score(X_test, y_test)\n",
        "print(neighbors, '\\n', train_accuracies, '\\n', test_accuracies)"
      ],
      "metadata": {
        "id": "OZpLr0-M0B_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing model complexity\n",
        "Now you have calculated the accuracy of the KNN model on the training and test sets using various values of n_neighbors, you can create a model complexity curve to visualize how performance changes as the model becomes less complex!\n",
        "\n",
        "The variables neighbors, train_accuracies, and test_accuracies, which you generated in the previous exercise, have all been preloaded for you. You will plot the results to aid in finding the optimal number of neighbors for your model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add a title \"KNN: Varying Number of Neighbors\".\n",
        "Plot the .values() method of train_accuracies on the y-axis against neighbors on the x-axis, with a label of \"Training Accuracy\".\n",
        "Plot the .values() method of test_accuracies on the y-axis against neighbors on the x-axis, with a label of \"Testing Accuracy\".\n",
        "Display the plot."
      ],
      "metadata": {
        "id": "CtrPm8VC0EIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a title\n",
        "plt.title(\"KNN: Varying Number of Neighbors\")\n",
        "\n",
        "# Plot training accuracies\n",
        "plt.plot(neighbors, train_accuracies.values(), label=\"Training Accuracy\")\n",
        "\n",
        "# Plot test accuracies\n",
        "plt.plot(neighbors, test_accuracies.values(), label=\"Testing Accuracy\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of Neighbors\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-nYPo_ny0JSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating features\n",
        "In this chapter, you will work with a dataset called sales_df, which contains information on advertising campaign expenditure across different media types, and the number of dollars generated in sales for the respective campaign. The dataset has been preloaded for you. Here are the first two rows:\n",
        "\n",
        "     tv        radio      social_media    sales\n",
        "1    13000.0   9237.76    2409.57         46677.90\n",
        "2    41000.0   15886.45   2913.41         150177.83\n",
        "You will use the advertising expenditure as features to predict sales values, initially working with the \"radio\" column. However, before you make any predictions you will need to create the feature and target arrays, reshaping them to the correct format for scikit-learn.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create X, an array of the values from the sales_df DataFrame's \"radio\" column.\n",
        "Create y, an array of the values from the sales_df DataFrame's \"sales\" column.\n",
        "Reshape X into a two-dimensional NumPy array.\n",
        "Print the shape of X and y."
      ],
      "metadata": {
        "id": "QyRrHKni0K1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create X from the radio column's values\n",
        "X = sales_df[\"radio\"].values\n",
        "\n",
        "# Create y from the sales column's values\n",
        "y = sales_df[\"sales\"].values\n",
        "\n",
        "# Reshape X\n",
        "X = X.reshape(-1,1)\n",
        "\n",
        "# Check the shape of the features and targets\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "RzljfnxO0Q1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a linear regression model\n",
        "Now you have created your feature and target arrays, you will train a linear regression model on all feature and target values.\n",
        "\n",
        "As the goal is to assess the relationship between the feature and target values there is no need to split the data into training and test sets.\n",
        "\n",
        "X and y have been preloaded for you as follows:\n",
        "\n",
        "y = sales_df[\"sales\"].values\n",
        "X = sales_df[\"radio\"].values.reshape(-1, 1)\n",
        "Instructions\n",
        "100 XP\n",
        "Import LinearRegression.\n",
        "Instantiate a linear regression model.\n",
        "Predict sales values using X, storing as predictions."
      ],
      "metadata": {
        "id": "stTgJHkp0ThG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LinearRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create the model\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "reg.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = reg.predict(X)\n",
        "\n",
        "print(predictions[:5])"
      ],
      "metadata": {
        "id": "YkcDW6te0ZHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing a linear regression model\n",
        "Now you have built your linear regression model and trained it using all available observations, you can visualize how well the model fits the data. This allows you to interpret the relationship between radio advertising expenditure and sales values.\n",
        "\n",
        "The variables X, an array of radio values, y, an array of sales values, and predictions, an array of the model's predicted values for y given X, have all been preloaded for you from the previous exercise.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import matplotlib.pyplot as plt.\n",
        "Create a scatter plot visualizing y against X, with observations in blue.\n",
        "Draw a red line plot displaying the predictions against X.\n",
        "Display the plot."
      ],
      "metadata": {
        "id": "vBIplTkg0eAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create scatter plot\n",
        "plt.scatter(X, y, color=\"blue\")\n",
        "\n",
        "# Create line plot\n",
        "plt.plot(X, predictions, color=\"red\")\n",
        "plt.xlabel(\"Radio Expenditure ($)\")\n",
        "plt.ylabel(\"Sales ($)\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "coR5gKlh0gHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fit and predict for regression\n",
        "Now you have seen how linear regression works, your task is to create a multiple linear regression model using all of the features in the sales_df dataset, which has been preloaded for you. As a reminder, here are the first two rows:\n",
        "\n",
        "     tv        radio      social_media    sales\n",
        "1    13000.0   9237.76    2409.57         46677.90\n",
        "2    41000.0   15886.45   2913.41         150177.83\n",
        "You will then use this model to predict sales based on the values of the test features.\n",
        "\n",
        "LinearRegression and train_test_split have been preloaded for you from their respective modules.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create X, an array containing values of all features in sales_df, and y, containing all values from the \"sales\" column.\n",
        "Instantiate a linear regression model.\n",
        "Fit the model to the training data.\n",
        "Create y_pred, making predictions for sales using the test features."
      ],
      "metadata": {
        "id": "9O8cD8lL0lew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X and y arrays\n",
        "X = sales_df.drop(\"sales\", axis=1).values\n",
        "y = sales_df[\"sales\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Instantiate the model\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Fit the model to the data\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"Predictions: {}, Actual Values: {}\".format(y_pred[:2], y_test[:2]))"
      ],
      "metadata": {
        "id": "B2KWvw6o0ubM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression performance\n",
        "Now you have fit a model, reg, using all features from sales_df, and made predictions of sales values, you can evaluate performance using some common regression metrics.\n",
        "\n",
        "The variables X_train, X_test, y_train, y_test, and y_pred, along with the fitted model, reg, all from the last exercise, have been preloaded for you.\n",
        "\n",
        "Your task is to find out how well the features can explain the variance in the target values, along with assessing the model's ability to make predictions on unseen data.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import mean_squared_error.\n",
        "Calculate the model's R-squared score by passing the test feature values and the test target values to an appropriate method.\n",
        "Calculate the model's root mean squared error using y_test and y_pred.\n",
        "Print r_squared and rmse."
      ],
      "metadata": {
        "id": "ViDv0PRR0wSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Compute R-squared\n",
        "r_squared = reg.score(X_test, y_test)\n",
        "\n",
        "# Compute RMSE\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "\n",
        "# Print the metrics\n",
        "print(\"R^2: {}\".format(r_squared))\n",
        "print(\"RMSE: {}\".format(rmse))"
      ],
      "metadata": {
        "id": "Ocl3J3pp003n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation for R-squared\n",
        "Cross-validation is a vital approach to evaluating a model. It maximizes the amount of data that is available to the model, as the model is not only trained but also tested on all of the available data.\n",
        "\n",
        "In this exercise, you will build a linear regression model, then use 6-fold cross-validation to assess its accuracy for predicting sales using social media advertising expenditure. You will display the individual score for each of the six-folds.\n",
        "\n",
        "The sales_df dataset has been split into y for the target variable, and X for the features, and preloaded for you. LinearRegression has been imported from sklearn.linear_model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import KFold and cross_val_score.\n",
        "Create kf by calling KFold(), setting the number of splits to six, shuffle to True, and setting a seed of 5.\n",
        "Perform cross-validation using reg on X and y, passing kf to cv.\n",
        "Print the cv_scores."
      ],
      "metadata": {
        "id": "6R5QdhYK04Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "# Create a KFold object\n",
        "kf = KFold(n_splits=6, shuffle=True, random_state=5)\n",
        "\n",
        "reg = LinearRegression()\n",
        "\n",
        "# Compute 6-fold cross-validation scores\n",
        "cv_scores = cross_val_score(reg, X, y, cv=kf)\n",
        "\n",
        "# Print scores\n",
        "print(cv_scores)"
      ],
      "metadata": {
        "id": "-8Rp0lAY06tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing cross-validation metrics\n",
        "Now you have performed cross-validation, it's time to analyze the results.\n",
        "\n",
        "You will display the mean, standard deviation, and 95% confidence interval for cv_results, which has been preloaded for you from the previous exercise.\n",
        "\n",
        "numpy has been imported for you as np.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate and print the mean of the results.\n",
        "Calculate and print the standard deviation of cv_results.\n",
        "Display the 95% confidence interval for your results using np.quantile()."
      ],
      "metadata": {
        "id": "5_pkF4WT07z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the mean\n",
        "print(np.mean(cv_results))\n",
        "\n",
        "# Print the standard deviation\n",
        "print(np.std(cv_results))\n",
        "\n",
        "# Print the 95% confidence interval\n",
        "print(np.quantile(cv_results, [0.025, 0.975]))"
      ],
      "metadata": {
        "id": "UHu7jevZ1B-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularized regression: Ridge\n",
        "Ridge regression performs regularization by computing the squared values of the model parameters multiplied by alpha and adding them to the loss function.\n",
        "\n",
        "In this exercise, you will fit ridge regression models over a range of different alpha values, and print their\n",
        " scores. You will use all of the features in the sales_df dataset to predict \"sales\". The data has been split into X_train, X_test, y_train, y_test for you.\n",
        "\n",
        "A variable called alphas has been provided as a list containing different alpha values, which you will loop through to generate scores.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import Ridge.\n",
        "Instantiate Ridge, setting alpha equal to alpha.\n",
        "Fit the model to the training data.\n",
        "Calculate the\n",
        " score for each iteration of ridge."
      ],
      "metadata": {
        "id": "6tNcPixW1UVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Ridge\n",
        "from sklearn.linear_model import Ridge\n",
        "alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
        "ridge_scores = []\n",
        "for alpha in alphas:\n",
        "\n",
        "  # Create a Ridge regression model\n",
        "  ridge = Ridge(alpha=alpha)\n",
        "\n",
        "  # Fit the data\n",
        "  ridge.fit(X_train, y_train)\n",
        "\n",
        "  # Obtain R-squared\n",
        "  score = ridge.score(X_test, y_test)\n",
        "  ridge_scores.append(score)\n",
        "print(ridge_scores)"
      ],
      "metadata": {
        "id": "B7Spgalc1Xj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso regression for feature importance\n",
        "In the video, you saw how lasso regression can be used to identify important features in a dataset.\n",
        "\n",
        "In this exercise, you will fit a lasso regression model to the sales_df data and plot the model's coefficients.\n",
        "\n",
        "The feature and target variable arrays have been pre-loaded as X and y, along with sales_columns, which contains the dataset's feature names.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import Lasso from sklearn.linear_model.\n",
        "Instantiate a Lasso regressor with an alpha of 0.3.\n",
        "Fit the model to the data.\n",
        "Compute the model's coefficients, storing as lasso_coef."
      ],
      "metadata": {
        "id": "Uxv1S0Wi1ZNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Lasso\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Instantiate a lasso regression model\n",
        "lasso = Lasso(alpha=0.3)\n",
        "\n",
        "# Fit the model to the data\n",
        "lasso.fit(X, y)\n",
        "\n",
        "# Compute and print the coefficients\n",
        "lasso_coef = lasso.coef_\n",
        "print(lasso_coef)\n",
        "plt.bar(sales_columns, lasso_coef)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "szG4WQMW1dAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessing a diabetes prediction classifier\n",
        "In this chapter you'll work with the diabetes_df dataset introduced previously.\n",
        "\n",
        "The goal is to predict whether or not each individual is likely to have diabetes based on the features body mass index (BMI) and age (in years). Therefore, it is a binary classification problem. A target value of 0 indicates that the individual does not have diabetes, while a value of 1 indicates that the individual does have diabetes.\n",
        "\n",
        "diabetes_df has been preloaded for you as a pandas DataFrame and split into X_train, X_test, y_train, and y_test. In addition, a KNeighborsClassifier() has been instantiated and assigned to knn.\n",
        "\n",
        "You will fit the model, make predictions on the test set, then produce a confusion matrix and classification report.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import confusion_matrix and classification_report.\n",
        "Fit the model to the training data.\n",
        "Predict the labels of the test set, storing the results as y_pred.\n",
        "Compute and print the confusion matrix and classification report for the test labels versus the predicted labels.\n"
      ],
      "metadata": {
        "id": "0K1XQf1KyXV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "# Fit the model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test data: y_pred\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "0mTq0gA31jwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a logistic regression model\n",
        "In this exercise, you will build a logistic regression model using all features in the diabetes_df dataset. The model will be used to predict the probability of individuals in the test set having a diabetes diagnosis.\n",
        "\n",
        "The diabetes_df dataset has been split into X_train, X_test, y_train, and y_test, and preloaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import LogisticRegression.\n",
        "Instantiate a logistic regression model, logreg.\n",
        "Fit the model to the training data.\n",
        "Predict the probabilities of each individual in the test set having a diabetes diagnosis, storing the array of positive probabilities as y_pred_probs."
      ],
      "metadata": {
        "id": "5ftw09x_q_G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate the model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_probs = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(y_pred_probs[:10])"
      ],
      "metadata": {
        "id": "RxUtcAtGrCZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC Curve\n",
        "Now you have built a logistic regression model for predicting diabetes status, you can plot the ROC curve to visualize how the true positive rate and false positive rate vary as the decision threshold changes.\n",
        "\n",
        "The test labels, y_test, and the predicted probabilities of the test features belonging to the positive class, y_pred_probs, have been preloaded for you, along with matplotlib.pyplot as plt.\n",
        "\n",
        "You will create a ROC curve and then interpret the results.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Import roc_curve.\n",
        "Calculate the ROC curve values, using y_test and y_pred_probs, and unpacking the results into fpr, tpr, and thresholds.\n",
        "Plot true positive rate against false positive rate.\n",
        "\n",
        "Q2: The model now looks better than randomly guessing numbers."
      ],
      "metadata": {
        "id": "UrlgdW-TqxO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import roc_curve\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Generate ROC curve values: fpr, tpr, thresholds\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "\n",
        "# Plot tpr against fpr\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Diabetes Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WpUAust6q2Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC AUC\n",
        "The ROC curve you plotted in the last exercise looked promising.\n",
        "\n",
        "Now you will compute the area under the ROC curve, along with the other classification metrics you have used previously.\n",
        "\n",
        "The confusion_matrix and classification_report functions have been preloaded for you, along with the logreg model you previously built, plus X_train, X_test, y_train, y_test. Also, the model's predicted test set labels are stored as y_pred, and probabilities of test set observations belonging to the positive class stored as y_pred_probs.\n",
        "\n",
        "A knn model has also been created and the performance metrics printed in the console, so you can compare the roc_auc_score, confusion_matrix, and classification_report between the two models.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import roc_auc_score.\n",
        "Calculate and print the ROC AUC score, passing the test labels and the predicted positive class probabilities.\n",
        "Calculate and print the confusion matrix.\n",
        "Call classification_report()."
      ],
      "metadata": {
        "id": "o2_wBNjtq4bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import roc_auc_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Calculate roc_auc_score\n",
        "print(roc_auc_score(y_test, y_pred_probs))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Calculate the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "BgGjdi_ArOJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
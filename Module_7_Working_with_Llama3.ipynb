{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIsKGUUv7d/RTNt5I6tdU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_7_Working_with_Llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 7 Starts here"
      ],
      "metadata": {
        "id": "P4Ezs3Yjnw4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and using Llama 3\n",
        "You are tasked with testing and evaluating the quality of the new Llama model that your company wants to use.\n",
        "\n",
        "To conduct these tests, you need to write code that will let you conduct completions on the Llama model, first by loading the model and then generating completions using the llama-cpp-python library. Anytime you interact with an LLM application, a starting point is to use the Llama class and a model of choice to generate text.\n",
        "\n",
        "As a check to make sure that the loading script works, you want to say \"Hello\" to the model and be able to see its reply.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the Llama class.\n",
        "Instantiate the Llama class, passing it the file path stored in path_to_model.\n",
        "Run a completion on the model using the instance of the Llama class in llm with the prompt \"Hello\"."
      ],
      "metadata": {
        "id": "CMNQVCng1dFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bxBkjzinprN"
      },
      "outputs": [],
      "source": [
        "# Load the correct class from the library\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Instantiate the model class\n",
        "llm = Llama(model_path=path_to_model, n_gpu_layers=-1)\n",
        "\n",
        "# Call the model with the prompt \"Hello\"\n",
        "output = llm(\"Hello\", max_tokens=32, stop=[\"Q: \", \"\\n\"])\n",
        "print(output)"
      ]
    }
  ]
}
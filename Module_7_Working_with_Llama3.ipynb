{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUdTKhSlCm2cdJ8bx7Tzar",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_7_Working_with_Llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 7 Starts here"
      ],
      "metadata": {
        "id": "P4Ezs3Yjnw4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and using Llama 3\n",
        "You are tasked with testing and evaluating the quality of the new Llama model that your company wants to use.\n",
        "\n",
        "To conduct these tests, you need to write code that will let you conduct completions on the Llama model, first by loading the model and then generating completions using the llama-cpp-python library. Anytime you interact with an LLM application, a starting point is to use the Llama class and a model of choice to generate text.\n",
        "\n",
        "As a check to make sure that the loading script works, you want to say \"Hello\" to the model and be able to see its reply.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the Llama class.\n",
        "Instantiate the Llama class, passing it the file path stored in path_to_model.\n",
        "Run a completion on the model using the instance of the Llama class in llm with the prompt \"Hello\"."
      ],
      "metadata": {
        "id": "CMNQVCng1dFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bxBkjzinprN"
      },
      "outputs": [],
      "source": [
        "# Load the correct class from the library\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Instantiate the model class\n",
        "llm = Llama(model_path=path_to_model, n_gpu_layers=-1)\n",
        "\n",
        "# Call the model with the prompt \"Hello\"\n",
        "output = llm(\"Hello\", max_tokens=32, stop=[\"Q: \", \"\\n\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing Llama 3 completion outputs\n",
        "Your company wants to use the Llama models in its Bronx Zoo question-answering bot for the animal exhibits.\n",
        "\n",
        "Your task is to extract the model's completion from the result stored in output. The output contains the completion and many other metadata. An early step to evaluate the model is to ask Llama 3 a question, and figure out how to parse its output. You are given a Llama model preloaded in llm, and given the prompt which asks it to name five foods that llamas eat, with the result stored in output.\n",
        "\n",
        "You are tasked with parsing the result in output and only retrieve the string result of the completion and store it in completion_string.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Limit the number of tokens generated to a max of 20 tokens.\n",
        "Stop the generation if the completion produces a line break, ie '\\n'.\n",
        "Parse the output variable and store the completion string in a new variable, completion_string."
      ],
      "metadata": {
        "id": "Zg-ps3FR2cTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "\t\"Q: Name 5 foods that llamas eat? A: \",\n",
        "  \t# restrict to 20 tokens\n",
        "\tmax_tokens=20,\n",
        "\t# add relevant stopping tokens\n",
        "\tstop = [\"Q:\", \"\\n\"],\n",
        ")\n",
        "# Retrieve the completion text and store in completion_string\n",
        "completion_string = output['choices'][0]['text'] if output['choices'] else \"\"\n",
        "print(completion_string)"
      ],
      "metadata": {
        "id": "bLD9MEEk2cnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
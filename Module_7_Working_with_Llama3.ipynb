{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTXBrfsWftljfPzBEB4CVH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_7_Working_with_Llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 7 Starts here"
      ],
      "metadata": {
        "id": "P4Ezs3Yjnw4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and using Llama 3\n",
        "You are tasked with testing and evaluating the quality of the new Llama model that your company wants to use.\n",
        "\n",
        "To conduct these tests, you need to write code that will let you conduct completions on the Llama model, first by loading the model and then generating completions using the llama-cpp-python library. Anytime you interact with an LLM application, a starting point is to use the Llama class and a model of choice to generate text.\n",
        "\n",
        "As a check to make sure that the loading script works, you want to say \"Hello\" to the model and be able to see its reply.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the Llama class.\n",
        "Instantiate the Llama class, passing it the file path stored in path_to_model.\n",
        "Run a completion on the model using the instance of the Llama class in llm with the prompt \"Hello\"."
      ],
      "metadata": {
        "id": "CMNQVCng1dFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bxBkjzinprN"
      },
      "outputs": [],
      "source": [
        "# Load the correct class from the library\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Instantiate the model class\n",
        "llm = Llama(model_path=path_to_model, n_gpu_layers=-1)\n",
        "\n",
        "# Call the model with the prompt \"Hello\"\n",
        "output = llm(\"Hello\", max_tokens=32, stop=[\"Q: \", \"\\n\"])\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing Llama 3 completion outputs\n",
        "Your company wants to use the Llama models in its Bronx Zoo question-answering bot for the animal exhibits.\n",
        "\n",
        "Your task is to extract the model's completion from the result stored in output. The output contains the completion and many other metadata. An early step to evaluate the model is to ask Llama 3 a question, and figure out how to parse its output. You are given a Llama model preloaded in llm, and given the prompt which asks it to name five foods that llamas eat, with the result stored in output.\n",
        "\n",
        "You are tasked with parsing the result in output and only retrieve the string result of the completion and store it in completion_string.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Limit the number of tokens generated to a max of 20 tokens.\n",
        "Stop the generation if the completion produces a line break, ie '\\n'.\n",
        "Parse the output variable and store the completion string in a new variable, completion_string."
      ],
      "metadata": {
        "id": "Zg-ps3FR2cTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "\t\"Q: Name 5 foods that llamas eat? A: \",\n",
        "  \t# restrict to 20 tokens\n",
        "\tmax_tokens=20,\n",
        "\t# add relevant stopping tokens\n",
        "\tstop = [\"Q:\", \"\\n\"],\n",
        ")\n",
        "# Retrieve the completion text and store in completion_string\n",
        "completion_string = output['choices'][0]['text'] if output['choices'] else \"\"\n",
        "print(completion_string)"
      ],
      "metadata": {
        "id": "bLD9MEEk2cnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More creative Llama completions\n",
        "You are a software developer working on integrating Llama in your company's chatbot pipelines. Unfortunately, the current Llama model you are using produces repetitive completions and often produces exactly the same results if you ask it the same question, which makes the bot feel less personable to your users.\n",
        "\n",
        "You decide to debug this issue by looking through the completion code and modify it so that the responses produced are more varied. The model is already instantiated with a model using llama_cpp and is stored in llm.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add the parameter and a corresponding value to the completion code so the model considers a wider variety of words during generation.\n",
        "Add the parameter to the completion code which penalizes the model for repeating the same words often."
      ],
      "metadata": {
        "id": "jeG-Cn7M-vgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "\t\t\"Q: Give me directions from grand central station to the Empire State building. A: \",\n",
        "  \t\t# Modify for the model to sample from more words\n",
        "\t\ttemperature=1.5,\n",
        "\t\trepeat_penalty=1.8,\n",
        "        max_tokens=15,\n",
        "        stop=[\"Q:\", \"\\n\"],\n",
        "        echo=False\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "8Gr2gGlr-v_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a philosophy chatbot\n",
        "You are a tester at a company building AI personas, and your task is to evaluate how well the new Llama models are able to generate completions in certain voices and styles.\n",
        "\n",
        "You will make a chatbot that thinks it's a philosopher and answers questions by pretending it is Plato. You are given a partially completed create_chat_completion call, which you will modify to make the chatbot respond to a user's question as if it was Plato himself.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fill in the dictionary in the first index of the messages list with the instructions to make the model respond as if it is the Greek philosopher Plato and the appropriate role.\n",
        "Fill in the dictionary in the second index of messages with the prompting question from the user and the appropriate role.\n",
        "Ensure that both your instruction, and the user's question are correctly passed to the function call."
      ],
      "metadata": {
        "id": "ZFga2chd-8J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = [\n",
        "\t# Instruct the model to behave like Plato\n",
        "\t\t{\"role\": \"system\", \"content\": \"You are the Greek philosopher Plato. Answer every question using his voice.\"},\n",
        "\t{\n",
        "          \t\"role\": \"user\",\n",
        "\t\t\t\"content\": \"Can any shape that exist in the real world be perfect and why?\"\n",
        "    }\n",
        "]\n",
        "# Pass in conversation context to the completion call\n",
        "result = llm.create_chat_completion(messages=history, max_tokens=20)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "QzcPpKeZ-8fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Llama speak like a pirate\n",
        "Your task is to create a prompt for a Llama model to serve as the language backend for an interactive pirate robot at Disney World. Ensure the model's output is always in a pirate voice and includes \"Aye Matey\" in its response. Create an appropriate instruction for this prompt, using keywords to guide the model's output.\n",
        "\n",
        "The Llama class has already been instantiated in the llm variable and the code to call the completion is provided.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Include the appropriate keywords in the prompt in the correct locations: Instruction:, Question:, and Response: and ensure the instruction includes some directive on including \"Aye Matey\" in the model response and to make the model have a pirate voice."
      ],
      "metadata": {
        "id": "mRSpPj2UDjKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the keywords and instructions in the correct locations in the following prompt\n",
        "text=\"\"\"Instruction: You are an assistant who only speaks like a Pirate. Aye Maytey.\n",
        "Question: How long does it take to go around the Earth once?\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "output = llm(\n",
        "      text,\n",
        "      max_tokens=15,\n",
        "      stop=[\"Q:\", \"\\n\"],\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "ya3czY4BDjjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3-shot prompting with Llama\n",
        "You work at a food delivery company as a data analyst, and you are investigating the sentiment (positive or negative) people have about your company from reviews on Google and Yelp.\n",
        "\n",
        "Since you don't want to train a classification model from scratch to identify the reviews as positive or negative, you decide to create a prompt that you will feed to your instance of Llama 3. You decide to use few shot learning by writing three examples with the review and the sentiment, and use the model identify the sentiment on the 4th example, which you will replace with each review you collected.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a prompt using a few-shot prompting template with 3 examples."
      ],
      "metadata": {
        "id": "Gjz-2ExOFSNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in the 3-shot prompt (you can use multiple lines)\n",
        "text=\"\"\"\n",
        "Review 1: This food is not as good as the pictures...\n",
        "Sentiment 1: This is a negative review.\n",
        "Reivew 2: Wow! Astonishing flavor. Plus the chef and waiters are hot too!\n",
        "Sentiment 2: A positive response, complimenting the food, as well as the looks of the staff.\n",
        "Review 3: I wish I can take home the hot waiter, but I guess the good food will do.\n",
        "Review 3: Positive response. Complimenting excellent food, as well as wanting the hot male waiter\n",
        "Review 4: Delicious food, and excellent customer service! I wish I can eat the waiter too though...\n",
        "Sentiment 4:\"\"\"\n",
        "\n",
        "output = llm(text, max_tokens=32,stop=[\"Q:\"])\n",
        "\n",
        "print(output['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "ktq_c_DKFSo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a JSON inventory list\n",
        "You are asked to use an LLM to produce a structured JSON with a list of items and their count to help a supermarket automate their inventory process.\n",
        "\n",
        "The model takes a text description of the inventory as input and produces the JSON as output. This feature of the inventory management system automatically extracts inventory data from natural language and stores it in a structured format for downstream tasks.\n",
        "\n",
        "You are provided with the llm class instance with a Llama model pre-loaded and the system prompt to get you started.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Specify the parameters in create_chat_completion that lets you generate responses in JSON format."
      ],
      "metadata": {
        "id": "Dr7tv-lXQ5bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant processing lists from text to JSON format: you extract item counts from text and output it in JSON with the item name as the key and the number of that item as the value\",},\n",
        "            {\"role\": \"user\", \"content\": \"I have fifteen apples, thirty-three oranges, and five thousand fifty-two potatoes.\"},\n",
        "        ],\n",
        "\t\t# Specify output format to JSON\n",
        "        response_format={\"type\": \"json_object\"},\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "hC3ObCTIQ51J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating answers with a JSON schema\n",
        "You are part of a team working on an online education platform. In a course teaching about space, there is an interactive exercise where students are able to ask questions about a planet and the answer is shown on their screen through a graphical view. This question-answering feature is powered by an LLM, but the graphical view requires a JSON as an input with the fields Question and Answer to correctly showing the question and answer.\n",
        "\n",
        "You believe that using the new Llama models and llama-cpp-python, you can get the LLM to produce the answer and format it into the correct JSON schema in one step.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add the field to specify a JSON schema in response_format and the properties it may have.\n",
        "Specify the Question and Answer fields in the schema with the string type.\n",
        "Specify the required fields in the schema."
      ],
      "metadata": {
        "id": "-tlORVu2TYPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about space. You return your results in a JSON format with the Question and Answer fields.\",},\n",
        "            {\"role\": \"user\", \"content\": \"How old is the Milky Way Galaxy?\"},\n",
        "        ],\n",
        "        response_format={\n",
        "            \"type\": \"json_object\",\n",
        "          \t# Set the keyword that lets you specify a schema\n",
        "            \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            # Set the properties of the JSON fields and their data types\n",
        "            \"properties\": {\"Question\": {\"type\": \"string\"}, \"Answer\": {\"type\": \"string\"}},\n",
        "            # Declare the required JSON fields here\n",
        "            \"required\": [\"Question\", \"Answer\"],\n",
        "            },\n",
        "        },\n",
        ")\n",
        "\n",
        "print(output['choices'][0]['message']['content'])"
      ],
      "metadata": {
        "id": "CI8URBeyTYvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making safe responses\n",
        "Your team is working on a chatbot powered by a Llama 3 model that will be used in a critical insurance system that medical staff will interact with to consult which medications they can provide based on the patient's policy.\n",
        "\n",
        "Your system is subject to audits and as a requirement the system also has to be deterministic, meaning your language model's outputs need to be consistent and predictable. So, if the same text is entered twice, the model will produce the exact same results each time.\n",
        "\n",
        "You have been provided the Llama class instance in the llm variable and the code to call the completion. You are also given a sample prompt to test with.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Modify the completion code so that at most 10 tokens are generated.\n",
        "Restrict the completion decoding so that it only ever chooses between the two most likely tokens at each completion step."
      ],
      "metadata": {
        "id": "U2oQt7T7T902"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output1 = llm(\n",
        "\t\t\"What are the symptoms of strep throat?\",\n",
        "  \t\t# Set the maximum number of tokens\n",
        "      \tmax_tokens = 10,\n",
        "\t\t# Restrict decoding to choose between top two tokens\n",
        "\t\ttop_k = 50\n",
        ")\n",
        "\n",
        "print(output1['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "nLX7SEndT-L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a creative chatbot\n",
        "You are building a chatbot to help customers brainstorm new ideas and address writer's block. To that end, your bot needs to be creative, and able to answer the same queries but often produce a diverse set of responses for the same queries.\n",
        "\n",
        "You have been provided the Llama class instance in the llm variable and the code to call the completion. You are also given a sample prompt to test with.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Complete the code to run a completion, and adjust the top-p decoding parameter so that it considers the top 80% cumulative probable words in the token vocabulary.\n",
        "\n"
      ],
      "metadata": {
        "id": "KX_HTsjWUYte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output1 = llm(\n",
        "      \t\"What are three names you could give a pirate ship whose crew is looking for an elusive treasure known as the One Piece?\",\n",
        "\t\tmax_tokens=15,\n",
        "\t\t# Add the decoding parameter and corresponding value\n",
        "\t\ttop_p = 0.07\n",
        "\t)\n",
        "\n",
        "print(output1['choices'][0]['text'])"
      ],
      "metadata": {
        "id": "bQmSZgttUZGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Personal shopping agent\n",
        "\n",
        "You are tasked with creating a fashion recommendation bot that helps people find new outfits to wear at different events. Your team is developing a bot for shopping recommendations using an Agent class to encapsulate the LLM details. They have tasked you with creating a good system prompt that will help the agent behave like a fashion expert with the name 'Ivy Verlaine'.\n",
        "\n",
        "You are given a pre-loaded Llama model instantiated in llm and the Agent class to get you started. The Agent class is instantiated with an LLM, a system prompt, and conversation history.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a system prompt that instructs the model to behave like a fashion expert.\n",
        "Instantiate an Agent class in fashion_agent with your system prompt and the pre-loaded llm."
      ],
      "metadata": {
        "id": "8vej2sYAf3CK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the instruction to the system\n",
        "instruction = \"You are a fashion expert. Use Anna Wintour's voice and judegment.\"\n",
        "\n",
        "# Instantiate the Agent class with the llm and the system prompt\n",
        "fashion_agent = Agent(llm, system_prompt=instruction)\n",
        "\n",
        "result = fashion_agent.create_completion(\"I'm going to a wedding, what should I wear?\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "83VXEB70f5X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-agent conversations\n",
        "You are an LLM researcher trying to optimize how you create prompts to instruct a model used in a tutoring bot. To make it easier to iterate through many prompts for your bot's LLM, you've decided to use two instances of the Agent class, teacher_agent and student_agent.\n",
        "\n",
        "teacher_agent is used to generate instructions, which will become the instructions that form part of the student_agent's prompt.\n",
        "\n",
        "The student_agent is the actual agent that you want to use in your tutoring bot, ie. it has to behave like a tutor.\n",
        "\n",
        "You want to use the teacher_agent to quickly iterate over many system prompt variations for your student_agent so that you use the best possible system prompt for the student_agent.\n",
        "\n",
        "You are given a pre-loaded Llama model instantiated in llm and the Agent class.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a user prompt for teacher_agent to help it generate a completion that instructs how to be a good tutor so that it can be used as the system prompt for the student_agent.\n",
        "Set an appropriate token limit to the instructions generated by teacher_agent.\n",
        "Instantiate the student_agent with the generated prompt from teacher_agent and run the test completion."
      ],
      "metadata": {
        "id": "onp7JzyghcTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_agent = Agent(llm, system_prompt=\"You provide instruction, concisely and step-by-step, on how to be a good tutor for any high school subject.\")\n",
        "instruction = teacher_agent.create_completion(\n",
        "  \t\t\t\t\t\t\t\t\t\t\t# Add a user prompt\n",
        "  \t\t\t\t\t\t\t\t\t\t\tuser_prompt='',\n",
        "  \t\t\t\t\t\t\t\t\t\t\t# Set token limit\n",
        "  \t\t\t\t\t\t\t\t\t\t\tmax_tokens=20)\n",
        "\n",
        "# Use the completion from teacher_agent as the system prompt\n",
        "student_agent = Agent(llm, system_prompt=instruction)\n",
        "response = student_agent.create_completion(\"Can you explain to me how differentiation works?\", max_tokens=100)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "vwdIeOZphcyP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
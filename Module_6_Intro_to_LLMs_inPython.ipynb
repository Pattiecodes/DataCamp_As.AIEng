{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH3lEEGnStVerNEXWbiP2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_6_Intro_to_LLMs_inPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6 starts here"
      ],
      "metadata": {
        "id": "chkJhSomOty3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a pipeline for summarization\n",
        "Run a summarization pipeline using the \"cnicu/t5-small-booksum\" model from the Hugging Face hub.\n",
        "\n",
        "A long_text about the Eiffel Tower has been provided and the pipeline module from transformers is already imported.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the model pipeline for a summarization task using the model \"cnicu/t5-small-booksum\".\n",
        "Generate the output by passing the long_text to the pipeline; limit the output to 50 tokens.\n",
        "Access and print the summarized text only from the output."
      ],
      "metadata": {
        "id": "we2pLiOLiCyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ok7JU8jOtBT"
      },
      "outputs": [],
      "source": [
        "# Load the model pipeline\n",
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
        "\n",
        "# Pass the long text to the model\n",
        "output = summarizer(long_text, max_length=50)\n",
        "\n",
        "# Access and print the summarized text\n",
        "print(output[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating text\n",
        "LLMs have many capabilities with text generation being one of the most popular.\n",
        "\n",
        "You need to generate a response to a customer review found in text; it contains the same customer review for the Riverview Hotel you've seen before.\n",
        "\n",
        "The pipeline module has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate the generator pipeline specifying an appropriate task for generating text.\n",
        "Complete the prompt by including the text and response in the f-string.\n",
        "Complete the model pipeline by specifying a maximum length of 150 tokens and setting the pad_token_id to the end-of-sequence token."
      ],
      "metadata": {
        "id": "m6kg09Gepf5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the pipeline\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
        "\n",
        "# Complete the prompt\n",
        "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Complete the model pipeline\n",
        "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "L_gADtAnpgNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translating text\n",
        "You've seen an example of English-to-Spanish translation. Now it's your turn to try it the other way around with Spanish-to-English translation.\n",
        "\n",
        "pipeline has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define the pipeline task for Spanish-to-English translation (es_to_en).\n",
        "Translate the spanish_text using the model pipeline."
      ],
      "metadata": {
        "id": "OVbIqo78pl_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_text = \"Este curso sobre LLMs se est√° poniendo muy interesante\"\n",
        "\n",
        "# Define the pipeline\n",
        "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the Spanish text\n",
        "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "id": "-pH3OZVDp5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the correct model structure\n",
        "Question-answering can be either extractive or generative, each requiring a different transformer structure to process input and output correctly.\n",
        "\n",
        "They use either:\n",
        "\n",
        "Encoder-only models such as \"distilbert-base-uncased-distilled-squad\"\n",
        "Decoder-only models such as \"gpt2\"\n",
        "Use your knowledge of common models for specific tasks to select the appropriate one. pipeline is loaded, as well as text on the Mona Lisa.\n",
        "\n",
        "Instructions 1/2\n",
        "Use an appropriate model for extractive question-answering."
      ],
      "metadata": {
        "id": "_6vOwf-xiz2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "output = qa(question=question, context=text)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "xH4CAGmfi1Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "Use an appropriate model for generative question-answering."
      ],
      "metadata": {
        "id": "_AfpqzKhjEi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "output = qa({\"context\": text, \"question\": question}, max_length=150)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "Huky0qtkjD64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing text\n",
        "You want to leverage a pre-trained model from Hugging Face and fine-tune it with data from your company support team to help classify interactions depending on the risk for churn. This will help the team prioritize what to address first, and how to address it, making them more proactive.\n",
        "\n",
        "Prepare the training and test data for fine-tuning by tokenizing the text.\n",
        "\n",
        "The data AutoTokenizer and AutoModelForSequenceClassification have been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the pre-trained model and tokenizer in preparation for fine-tuning.\n",
        "Tokenize both the train_data[\"interaction\"] and test_data[\"interaction\"], enabling padding and sequence truncation."
      ],
      "metadata": {
        "id": "hI1MSVAxqm63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize the data\n",
        "tokenized_training_data = tokenizer(train_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
        "\n",
        "tokenized_test_data = tokenizer(test_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
        "\n",
        "print(tokenized_training_data)"
      ],
      "metadata": {
        "id": "XrLHqE7uqnbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping tokenization\n",
        "You now want to test out having more control over the tokenization and want to try tokenizing the data in rows or batches. This will also give you a result that is a DataSet object, which you'll need for training.\n",
        "\n",
        "The tokenizer has been loaded for you along with the data as train_data and test_data.\n",
        "\n",
        "Instructions 1/2\n",
        "Complete tokenize_function returning tokenized tensors with sequence truncation and tokenize the train_data in batches."
      ],
      "metadata": {
        "id": "QkuivJwwrKRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the function\n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"interaction\"],\n",
        "                     return_tensors=\"pt\",\n",
        "                     padding=True,\n",
        "                     truncation=True,\n",
        "                     max_length=64)\n",
        "\n",
        "# Tokenize in batches\n",
        "tokenized_in_batches = train_data.map(tokenize_function, batched=True)\n",
        "\n",
        "print(tokenized_in_batches)"
      ],
      "metadata": {
        "id": "3z4IX8_IrMCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "Apply tokenize_function to train_data and tokenize row by row."
      ],
      "metadata": {
        "id": "htJN9Yl7rYNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the function\n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"interaction\"],\n",
        "                     return_tensors=\"pt\",\n",
        "                     padding=True,\n",
        "                     truncation=True,\n",
        "                     max_length=64)\n",
        "\n",
        "# Tokenize row by row\n",
        "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
        "\n",
        "print(tokenized_by_row)"
      ],
      "metadata": {
        "id": "1Zzw9INWradB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up training arguments\n",
        "After tokenizing your customer support interactions, you need to set up your training arguments before you can fine-tuned a pre-trained model.\n",
        "\n",
        "TrainingArguments has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set up an instance of TrainingArguments().\n",
        "Set the evaluation strategy as \"epoch\".\n",
        "Specify three training epochs.\n",
        "Set the batch sizes for both training and evaluation as three.\n",
        "\n"
      ],
      "metadata": {
        "id": "75qMwcX5raxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up an instance of TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./finetuned\",\n",
        "  # Set the evaluation strategy\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  # Specify the number of epochs\n",
        "  num_train_epochs=3,\n",
        "  learning_rate=2e-5,\n",
        "  # Set the batch sizes\n",
        "  per_device_train_batch_size=3,\n",
        "  per_device_eval_batch_size=3,\n",
        "  weight_decay=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "-1QCPWKxrbNr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
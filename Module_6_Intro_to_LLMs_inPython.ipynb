{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPjg4k2MDW+BFyhnUQhu+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_6_Intro_to_LLMs_inPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6 starts here"
      ],
      "metadata": {
        "id": "chkJhSomOty3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a pipeline for summarization\n",
        "Run a summarization pipeline using the \"cnicu/t5-small-booksum\" model from the Hugging Face hub.\n",
        "\n",
        "A long_text about the Eiffel Tower has been provided and the pipeline module from transformers is already imported.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the model pipeline for a summarization task using the model \"cnicu/t5-small-booksum\".\n",
        "Generate the output by passing the long_text to the pipeline; limit the output to 50 tokens.\n",
        "Access and print the summarized text only from the output."
      ],
      "metadata": {
        "id": "we2pLiOLiCyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ok7JU8jOtBT"
      },
      "outputs": [],
      "source": [
        "# Load the model pipeline\n",
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
        "\n",
        "# Pass the long text to the model\n",
        "output = summarizer(long_text, max_length=50)\n",
        "\n",
        "# Access and print the summarized text\n",
        "print(output[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating text\n",
        "LLMs have many capabilities with text generation being one of the most popular.\n",
        "\n",
        "You need to generate a response to a customer review found in text; it contains the same customer review for the Riverview Hotel you've seen before.\n",
        "\n",
        "The pipeline module has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate the generator pipeline specifying an appropriate task for generating text.\n",
        "Complete the prompt by including the text and response in the f-string.\n",
        "Complete the model pipeline by specifying a maximum length of 150 tokens and setting the pad_token_id to the end-of-sequence token."
      ],
      "metadata": {
        "id": "m6kg09Gepf5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the pipeline\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
        "\n",
        "# Complete the prompt\n",
        "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Complete the model pipeline\n",
        "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "L_gADtAnpgNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OVbIqo78pl_-"
      }
    }
  ]
}
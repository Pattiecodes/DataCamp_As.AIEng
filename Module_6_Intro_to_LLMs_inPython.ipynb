{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu40uAdRnTFDiR8+8OyGrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_6_Intro_to_LLMs_inPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6 starts here"
      ],
      "metadata": {
        "id": "chkJhSomOty3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a pipeline for summarization\n",
        "Run a summarization pipeline using the \"cnicu/t5-small-booksum\" model from the Hugging Face hub.\n",
        "\n",
        "A long_text about the Eiffel Tower has been provided and the pipeline module from transformers is already imported.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the model pipeline for a summarization task using the model \"cnicu/t5-small-booksum\".\n",
        "Generate the output by passing the long_text to the pipeline; limit the output to 50 tokens.\n",
        "Access and print the summarized text only from the output."
      ],
      "metadata": {
        "id": "we2pLiOLiCyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ok7JU8jOtBT"
      },
      "outputs": [],
      "source": [
        "# Load the model pipeline\n",
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
        "\n",
        "# Pass the long text to the model\n",
        "output = summarizer(long_text, max_length=50)\n",
        "\n",
        "# Access and print the summarized text\n",
        "print(output[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating text\n",
        "LLMs have many capabilities with text generation being one of the most popular.\n",
        "\n",
        "You need to generate a response to a customer review found in text; it contains the same customer review for the Riverview Hotel you've seen before.\n",
        "\n",
        "The pipeline module has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate the generator pipeline specifying an appropriate task for generating text.\n",
        "Complete the prompt by including the text and response in the f-string.\n",
        "Complete the model pipeline by specifying a maximum length of 150 tokens and setting the pad_token_id to the end-of-sequence token."
      ],
      "metadata": {
        "id": "m6kg09Gepf5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the pipeline\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
        "\n",
        "# Complete the prompt\n",
        "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Complete the model pipeline\n",
        "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "L_gADtAnpgNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translating text\n",
        "You've seen an example of English-to-Spanish translation. Now it's your turn to try it the other way around with Spanish-to-English translation.\n",
        "\n",
        "pipeline has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define the pipeline task for Spanish-to-English translation (es_to_en).\n",
        "Translate the spanish_text using the model pipeline."
      ],
      "metadata": {
        "id": "OVbIqo78pl_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_text = \"Este curso sobre LLMs se est√° poniendo muy interesante\"\n",
        "\n",
        "# Define the pipeline\n",
        "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the Spanish text\n",
        "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "id": "-pH3OZVDp5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the correct model structure\n",
        "Question-answering can be either extractive or generative, each requiring a different transformer structure to process input and output correctly.\n",
        "\n",
        "They use either:\n",
        "\n",
        "Encoder-only models such as \"distilbert-base-uncased-distilled-squad\"\n",
        "Decoder-only models such as \"gpt2\"\n",
        "Use your knowledge of common models for specific tasks to select the appropriate one. pipeline is loaded, as well as text on the Mona Lisa.\n",
        "\n",
        "Instructions 1/2\n",
        "Use an appropriate model for extractive question-answering."
      ],
      "metadata": {
        "id": "_6vOwf-xiz2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "output = qa(question=question, context=text)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "xH4CAGmfi1Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "Use an appropriate model for generative question-answering."
      ],
      "metadata": {
        "id": "_AfpqzKhjEi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "output = qa({\"context\": text, \"question\": question}, max_length=150)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "Huky0qtkjD64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing text\n",
        "You want to leverage a pre-trained model from Hugging Face and fine-tune it with data from your company support team to help classify interactions depending on the risk for churn. This will help the team prioritize what to address first, and how to address it, making them more proactive.\n",
        "\n",
        "Prepare the training and test data for fine-tuning by tokenizing the text.\n",
        "\n",
        "The data AutoTokenizer and AutoModelForSequenceClassification have been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the pre-trained model and tokenizer in preparation for fine-tuning.\n",
        "Tokenize both the train_data[\"interaction\"] and test_data[\"interaction\"], enabling padding and sequence truncation."
      ],
      "metadata": {
        "id": "hI1MSVAxqm63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize the data\n",
        "tokenized_training_data = tokenizer(train_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
        "\n",
        "tokenized_test_data = tokenizer(test_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
        "\n",
        "print(tokenized_training_data)"
      ],
      "metadata": {
        "id": "XrLHqE7uqnbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq/Mm9qrx9Xtic82TNoW4u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_6_Intro_to_LLMs_inPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6 starts here"
      ],
      "metadata": {
        "id": "chkJhSomOty3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a pipeline for summarization\n",
        "Run a summarization pipeline using the \"cnicu/t5-small-booksum\" model from the Hugging Face hub.\n",
        "\n",
        "A long_text about the Eiffel Tower has been provided and the pipeline module from transformers is already imported.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the model pipeline for a summarization task using the model \"cnicu/t5-small-booksum\".\n",
        "Generate the output by passing the long_text to the pipeline; limit the output to 50 tokens.\n",
        "Access and print the summarized text only from the output."
      ],
      "metadata": {
        "id": "we2pLiOLiCyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ok7JU8jOtBT"
      },
      "outputs": [],
      "source": [
        "# Load the model pipeline\n",
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
        "\n",
        "# Pass the long text to the model\n",
        "output = summarizer(long_text, max_length=50)\n",
        "\n",
        "# Access and print the summarized text\n",
        "print(output[0][\"summary_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating text\n",
        "LLMs have many capabilities with text generation being one of the most popular.\n",
        "\n",
        "You need to generate a response to a customer review found in text; it contains the same customer review for the Riverview Hotel you've seen before.\n",
        "\n",
        "The pipeline module has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate the generator pipeline specifying an appropriate task for generating text.\n",
        "Complete the prompt by including the text and response in the f-string.\n",
        "Complete the model pipeline by specifying a maximum length of 150 tokens and setting the pad_token_id to the end-of-sequence token."
      ],
      "metadata": {
        "id": "m6kg09Gepf5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the pipeline\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
        "\n",
        "# Complete the prompt\n",
        "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Complete the model pipeline\n",
        "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id, truncation=True)\n",
        "\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "L_gADtAnpgNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translating text\n",
        "You've seen an example of English-to-Spanish translation. Now it's your turn to try it the other way around with Spanish-to-English translation.\n",
        "\n",
        "pipeline has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define the pipeline task for Spanish-to-English translation (es_to_en).\n",
        "Translate the spanish_text using the model pipeline."
      ],
      "metadata": {
        "id": "OVbIqo78pl_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_text = \"Este curso sobre LLMs se está poniendo muy interesante\"\n",
        "\n",
        "# Define the pipeline\n",
        "translator = pipeline(task=\"translation_es_to_en\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the Spanish text\n",
        "translations = translator(spanish_text, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(translations[0][\"translation_text\"])"
      ],
      "metadata": {
        "id": "-pH3OZVDp5hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the correct model structure\n",
        "Question-answering can be either extractive or generative, each requiring a different transformer structure to process input and output correctly.\n",
        "\n",
        "They use either:\n",
        "\n",
        "Encoder-only models such as \"distilbert-base-uncased-distilled-squad\"\n",
        "Decoder-only models such as \"gpt2\"\n",
        "Use your knowledge of common models for specific tasks to select the appropriate one. pipeline is loaded, as well as text on the Mona Lisa.\n",
        "\n",
        "Instructions 1/2\n",
        "Use an appropriate model for extractive question-answering."
      ],
      "metadata": {
        "id": "_6vOwf-xiz2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "output = qa(question=question, context=text)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "xH4CAGmfi1Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "Use an appropriate model for generative question-answering."
      ],
      "metadata": {
        "id": "_AfpqzKhjEi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Who painted the Mona Lisa?\"\n",
        "\n",
        "# Define the appropriate model\n",
        "qa = pipeline(task=\"question-answering\", model=\"gpt2\")\n",
        "\n",
        "input_text = f\"Context: {text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
        "\n",
        "output = qa({\"context\": text, \"question\": question}, max_length=150)\n",
        "print(output['answer'])"
      ],
      "metadata": {
        "id": "Huky0qtkjD64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing text\n",
        "You want to leverage a pre-trained model from Hugging Face and fine-tune it with data from your company support team to help classify interactions depending on the risk for churn. This will help the team prioritize what to address first, and how to address it, making them more proactive.\n",
        "\n",
        "Prepare the training and test data for fine-tuning by tokenizing the text.\n",
        "\n",
        "The data AutoTokenizer and AutoModelForSequenceClassification have been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the pre-trained model and tokenizer in preparation for fine-tuning.\n",
        "Tokenize both the train_data[\"interaction\"] and test_data[\"interaction\"], enabling padding and sequence truncation."
      ],
      "metadata": {
        "id": "hI1MSVAxqm63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize the data\n",
        "tokenized_training_data = tokenizer(train_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
        "\n",
        "tokenized_test_data = tokenizer(test_data[\"interaction\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=20)\n",
        "\n",
        "print(tokenized_training_data)"
      ],
      "metadata": {
        "id": "XrLHqE7uqnbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping tokenization\n",
        "You now want to test out having more control over the tokenization and want to try tokenizing the data in rows or batches. This will also give you a result that is a DataSet object, which you'll need for training.\n",
        "\n",
        "The tokenizer has been loaded for you along with the data as train_data and test_data.\n",
        "\n",
        "Instructions 1/2\n",
        "Complete tokenize_function returning tokenized tensors with sequence truncation and tokenize the train_data in batches."
      ],
      "metadata": {
        "id": "QkuivJwwrKRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the function\n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"interaction\"],\n",
        "                     return_tensors=\"pt\",\n",
        "                     padding=True,\n",
        "                     truncation=True,\n",
        "                     max_length=64)\n",
        "\n",
        "# Tokenize in batches\n",
        "tokenized_in_batches = train_data.map(tokenize_function, batched=True)\n",
        "\n",
        "print(tokenized_in_batches)"
      ],
      "metadata": {
        "id": "3z4IX8_IrMCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "Apply tokenize_function to train_data and tokenize row by row."
      ],
      "metadata": {
        "id": "htJN9Yl7rYNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the function\n",
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"interaction\"],\n",
        "                     return_tensors=\"pt\",\n",
        "                     padding=True,\n",
        "                     truncation=True,\n",
        "                     max_length=64)\n",
        "\n",
        "# Tokenize row by row\n",
        "tokenized_by_row = train_data.map(tokenize_function, batched=False)\n",
        "\n",
        "print(tokenized_by_row)"
      ],
      "metadata": {
        "id": "1Zzw9INWradB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up training arguments\n",
        "After tokenizing your customer support interactions, you need to set up your training arguments before you can fine-tuned a pre-trained model.\n",
        "\n",
        "TrainingArguments has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set up an instance of TrainingArguments().\n",
        "Set the evaluation strategy as \"epoch\".\n",
        "Specify three training epochs.\n",
        "Set the batch sizes for both training and evaluation as three.\n",
        "\n"
      ],
      "metadata": {
        "id": "75qMwcX5raxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up an instance of TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./finetuned\",\n",
        "  # Set the evaluation strategy\n",
        "  evaluation_strategy=\"epoch\",\n",
        "  # Specify the number of epochs\n",
        "  num_train_epochs=3,\n",
        "  learning_rate=2e-5,\n",
        "  # Set the batch sizes\n",
        "  per_device_train_batch_size=3,\n",
        "  per_device_eval_batch_size=3,\n",
        "  weight_decay=0.01\n",
        ")"
      ],
      "metadata": {
        "id": "-1QCPWKxrbNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the trainer\n",
        "With training arguments set up, you need to move on to the trainer before putting the fine-tuned model to use.\n",
        "\n",
        "TrainingArguments and Trainer have been loaded for you, as well as your previous model, tokenizer and training_args.\n",
        "\n",
        "Note: actual training has been disabled for this exercise so that it runs faster.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set up the Trainer() object.\n",
        "Assign the previously defined training arguments and tokenizer.\n",
        "Train the model."
      ],
      "metadata": {
        "id": "9UZi7k07sRXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the trainer object\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    # Assign the training arguments and tokenizer\n",
        "    args = training_args,\n",
        "    train_dataset=tokenized_training_data,\n",
        "    eval_dataset=tokenized_test_data,\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "IoXPQiBcsR1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the fine-tuned model\n",
        "The model has been fine-tuned. Now you ready to use on some new data and generate some classifications. Let's see how well your fine-tuned model does at tagging new interactions as either low or high risk for churn.\n",
        "\n",
        "Your fine-tuned model and the tokenizer have been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Tokenize the new data.\n",
        "Pass the tokenized inputs into the fine-tuned model, disabling gradients.\n",
        "Extract the new predictions."
      ],
      "metadata": {
        "id": "rmVHqnPwswNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = [\"I'd just like to say, I love the product! Thank you!\"]\n",
        "\n",
        "# Tokenize the new data\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Pass the tokenized inputs through the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**new_input)\n",
        "\n",
        "# Extract the new predictions\n",
        "predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "\n",
        "label_map = {0: \"Low risk\", 1: \"High risk\"}\n",
        "for i, predicted_label in enumerate(predicted_labels):\n",
        "    churn_label = label_map[predicted_label]\n",
        "    print(f\"\\n Input Text {i + 1}: {input_text[i]}\")\n",
        "    print(f\"Predicted Label: {predicted_label}\")"
      ],
      "metadata": {
        "id": "6waQtY4yswnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning with one-shot learning\n",
        "Transfer learning has many approaches, one of them being one-shot learning, where a model is trained using one example it has seen.\n",
        "\n",
        "Set up a one-shot learning example for the model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Complete the one-shot learning example by showing the sample review is Positive.\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "RZZn1vLgCYci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include an example in the input ext\n",
        "input_text = \"\"\"\n",
        "Text: \"The dinner we had was great and the service too.\"\n",
        "Classify the sentiment of this sentence as either positive or negative.\n",
        "Example:\n",
        "Text: \"The food was delicious\"\n",
        "Sentiment: Positive\n",
        "Text: \"The dinner we had was great and the service too.\"\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "\n",
        "# Apply the example to the model\n",
        "result = model(input_text, max_length=100)\n",
        "\n",
        "print(result[0][\"label\"])"
      ],
      "metadata": {
        "id": "tIH7dsYlCZL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading metrics with evaluate\n",
        "Metrics are needed to evaluate the performance of LLMs, similar to classic machine learning models. The evaluate library helps to understand the complexities of LLMs. Start by loading the metrics that can be used to assess classification models.\n",
        "\n",
        "The evaluate library has already been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the accuracy, precision, recall, and F1 score metrics."
      ],
      "metadata": {
        "id": "AZdQ-pjDYNXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")"
      ],
      "metadata": {
        "id": "JxwsSifeYNsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Describing metrics\n",
        "It's never a bad time to revise the definitions of some popular metrics.\n",
        "\n",
        "The evaluate library has been loaded for you, along with the four classification metrics: accuracy, precision, recall, and f1.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Print the description for each metric."
      ],
      "metadata": {
        "id": "M01Sg8jTaELX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain a description of each metric\n",
        "print(accuracy.description)\n",
        "print(precision.description)\n",
        "print(recall.description)\n",
        "print(f1.description)"
      ],
      "metadata": {
        "id": "kts23rLxaEgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "See what data type each metric needs as input."
      ],
      "metadata": {
        "id": "BIIOfqzLaRXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See the required data types\n",
        "print(f\"The required data types for accuracy are: {accuracy.features}.\")\n",
        "print(f\"The required data types for precision are: {precision.features}.\")\n",
        "print(f\"The required data types for recall are: {recall.features}.\")\n",
        "print(f\"The required data types for f1 are: {f1.features}.\")"
      ],
      "metadata": {
        "id": "riKhniyOaT3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using evaluate metrics\n",
        "It's time to evaluate your LLM that classifies customer support interactions. Picking up from where you left your fine-tuned model, you'll now use a new validation dataset to assess the performance of your model.\n",
        "\n",
        "Some interactions and their corresponding labels have been loaded for you as validate_text and validate_labels. The model and tokenizer are also loaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Extract the predicted labels from the model logits found in the outputs.\n",
        "Compute the four loaded metrics by comparing real (validate_labels) and predicted labels."
      ],
      "metadata": {
        "id": "lol7WDdTbc0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# Extract the new predictions\n",
        "predicted_labels = predicted_labels = torch.argmax(outputs.logits, dim=1).tolist()\n",
        "\n",
        "# Compute the metrics by comparing real and predicted labels\n",
        "print(accuracy.compute(references=validate_labels, predictions=predicted_labels))\n",
        "print(precision.compute(references=validate_labels, predictions=predicted_labels))\n",
        "print(recall.compute(references=validate_labels, predictions=predicted_labels))\n",
        "print(f1.compute(references=validate_labels, predictions=predicted_labels))"
      ],
      "metadata": {
        "id": "5ZL9RFrFbe-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: 100% Accuracy (and other metrics) means that there is something wrong, in real world terms"
      ],
      "metadata": {
        "id": "zgqvD_v1byI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating perplexity\n",
        "Try your had at generating text and evaluating the perplexity score.\n",
        "\n",
        "You've been provided some input_text that is the start of a sentence: \"Current trends show that by 2030 \".\n",
        "\n",
        "Use an LLM to generate the rest of the sentence.\n",
        "\n",
        "An AutoModelForCausalLM model and its tokenizer have been loaded for you as model and tokenizer variables.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Encode the input_text and pass it to the provided text generation model.\n",
        "Load and compute the mean_perplexity score on the generated text."
      ],
      "metadata": {
        "id": "7iCGaWALrKYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the input text, generate and decode it\n",
        "input_text_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "output = model.generate(input_text_ids, max_length=20)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Text: \", generated_text)\n",
        "\n",
        "# Load and compute the perplexity score\n",
        "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "results = perplexity.compute(model_id=\"gpt2\", predictions=generated_text)\n",
        "print(\"Perplexity: \", results['mean_perplexity'])"
      ],
      "metadata": {
        "id": "8v7cuVBorKvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU translations\n",
        "Let's get familiar with the BLEU metric.\n",
        "\n",
        "A pipeline based on the Helsinki-NLP Spanish-English translation model and the BLEU metric has been loaded for you, using evaluate.load(\"bleu\") from the evaluate library.\n",
        "\n",
        "Given the following inputs and references for evaluation:\n",
        "```\n",
        "input_sentence_1 = \"Hola, ¿cómo estás?\"\n",
        "\n",
        "reference_1 = [\n",
        "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
        "     ]\n",
        "\n",
        "input_sentences_2 = [\"Hola, ¿cómo estás?\", \"Estoy genial, gracias.\"]\n",
        "\n",
        "references_2 = [\n",
        "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
        "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
        "     ]\n",
        "```\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Pass the input sentence in input_sentence_1 to the translator, then calculate the BLEU metric using reference_1."
      ],
      "metadata": {
        "id": "7cpUmYeCssdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the first input sentence then calucate the BLEU metric for translation quality\n",
        "translated_output = translator(input_sentence_1)\n",
        "\n",
        "translated_sentence = translated_output if isinstance(translated_output, str) else translated_output[0]['translation_text']\n",
        "\n",
        "print(\"Translated:\", translated_sentence)\n",
        "\n",
        "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "guQLl4-KstAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "Repeat the process, passing in the input_sentences_2 list containing two Spanish sentences to translate."
      ],
      "metadata": {
        "id": "vkxYDvfUs5fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "translated_outputs = translator(input_sentences_2)\n",
        "\n",
        "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
        "print(predictions)\n",
        "\n",
        "results = bleu.compute(predictions=predictions, references=references_2)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "WOHKe9jbs7QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating with ROUGE\n",
        "ROUGE is commonly used to evaluate summarization tasks as it checks for similarities between predictions and references. You have been provided with a model-generated summary, predictions, and a references summary for validate. Calculate the scores to see how well the model performed.\n",
        "\n",
        "The evaluate library has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the ROUGE metric.\n",
        "Calculate the ROUGE scores between the predicted and reference summaries."
      ],
      "metadata": {
        "id": "LkBVrvAugoTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the rouge metric\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
        "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
        "\n",
        "# Calculate the rouge scores between the predicted and reference summaries\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "print(\"ROUGE results: \", results)"
      ],
      "metadata": {
        "id": "rO3cQj-Hgong"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating with METEOR\n",
        "METEOR excels at evaluating some of the more semantic features in text. It works similar to ROUGE by comparing a model-generated output to a reference output. You've been provided these texts as generated and reference; it's over to you to evaluate the score.\n",
        "\n",
        "The evaluate library has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Compute and print the METEOR score."
      ],
      "metadata": {
        "id": "KbMLhJkFhZyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "generated = [\"The burrow stretched forward like a narrow corridor for a while, then plunged abruptly downward, so quickly that Alice had no chance to stop herself before she was tumbling into an extremely deep shaft.\"]\n",
        "reference = [\"The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\"]\n",
        "\n",
        "# Compute and print the METEOR score\n",
        "results = meteor.compute(predictions=generated, references=reference)\n",
        "print(\"Meteor: \", results['meteor'])"
      ],
      "metadata": {
        "id": "3_D6YS1NhZZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating with EM\n",
        "Exact Match helps us evaluate models when it comes to extractive question and answering but looking for, you guessed it, exact matches! Once again, you have been provided some predictions and references for evaluation. The evaluate library has been loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the Exact Match metric.\n",
        "Compute the Exact Match score and print the results."
      ],
      "metadata": {
        "id": "Au2hzaTwh0Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hello"
      ],
      "metadata": {
        "id": "4zFbx9f0h2ro"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
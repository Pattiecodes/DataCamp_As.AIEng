{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfe+TrH1npVjuFRgFUCTIe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DataCamp_As.AIEng/blob/main/Module_3_IntroToDeepLearningWithPyTorch_Learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is for Module 3 of the \"AI for Data Scientists\" Track from DataCamp"
      ],
      "metadata": {
        "id": "h36CQ4Ez9rPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with PyTorch tensors\n",
        "Tensors are the primary data structure in PyTorch and will be the building blocks for our deep learning models. They share many similarities with NumPy arrays but have some unique attributes too.\n",
        "\n",
        "In this exercise, you'll practice creating a tensor from a Python list of temperature data from two weather stations. The Python list is named temperatures and has two sublists whose elements represent a different day each, with columns for readings from two stations.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Begin by importing PyTorch.\n",
        "Create a tensor from the Python list temperatures."
      ],
      "metadata": {
        "id": "_j7JCqDVcpo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "temperatures = [[72, 75, 78], [70, 73, 76]]\n",
        "\n",
        "# Create a tensor from temperatures\n",
        "temp_tensor = torch.tensor(temperatures)"
      ],
      "metadata": {
        "id": "XDalQSW1cscc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking and adding tensors\n",
        "While continuing your temperature data collection, you realize that the recorded temperatures are off by 2 degrees, so you need to add 2 degrees to the tensor of temperatures. Before adjusting the data, you want to verify the shape and type of the tensor, to make sure they are compatible to be added together. The torch library has been pre-imported.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Check the shape of the temperatures tensor.\n",
        "Check the type of the temperatures tensor.\n",
        "Add the temperatures and adjustment tensors."
      ],
      "metadata": {
        "id": "6U9gpBLUdllS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperatures = torch.tensor([[72, 75, 78], [70, 73, 76]])\n",
        "adjustment = torch.tensor([[2, 2, 2], [2, 2, 2]])\n",
        "\n",
        "# Check the shape of the temperatures tensor\n",
        "temp_shape = temperatures.shape\n",
        "print(\"Shape of temperatures:\", temp_shape)\n",
        "\n",
        "# Check the type of the temperatures tensor\n",
        "temp_type = temperatures.dtype\n",
        "print(\"Data type of temperatures:\", temp_type)\n",
        "\n",
        "# Adjust the temperatures by adding the adjustment tensor\n",
        "corrected_temperatures = torch.tensor(temperatures + adjustment)\n",
        "\n",
        "print(\"Corrected temperatures:\", corrected_temperatures)"
      ],
      "metadata": {
        "id": "x5mO5p1hdk5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating my first Neural Network"
      ],
      "metadata": {
        "id": "u4mCoBD4d37S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your first neural network\n",
        "In this exercise, you will implement a small neural network containing two linear layers. The first layer takes an eight-dimensional input, and the last layer outputs a one-dimensional tensor.\n",
        "\n",
        "The torch package and the torch.nn package have already been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a neural network of two linear layers that takes a tensor of dimensions\n",
        " as input, representing 8 features, and outputs a tensor of dimensions\n",
        ".\n",
        "Use any output dimension for the first layer you want."
      ],
      "metadata": {
        "id": "irupjEDLC8jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
        "\n",
        "# Implement a small neural network with two linear layers\n",
        "model = nn.Sequential(nn.Linear(8, 4),\n",
        "                      nn.Linear(4, 1)\n",
        "                     )\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "q4Yz0OqmC_OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The sigmoid and softmax functions\n",
        "The sigmoid and softmax functions are two of the most popular activation functions in deep learning. They are both usually used as the last step of a neural network. Sigmoid functions are used for binary classification problems, whereas softmax functions are often used for multiclass classification problems.\n",
        "\n",
        "Let's say that you have a neural network that returns the values contained in the score tensor as a pre-activation output. Apply the activation function corresponding to the use cases described to get the output.\n",
        "\n",
        "torch.nn is already imported as nn.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Create a sigmoid function and apply it on input_tensor to generate a probability for a binary classification task."
      ],
      "metadata": {
        "id": "VxE2Yn42XwwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor([[0.8]])\n",
        "\n",
        "# Create a sigmoid function and apply it on input_tensor\n",
        "sigmoid = nn.Sigmoid()\n",
        "probability = sigmoid(input_tensor)\n",
        "print(probability)"
      ],
      "metadata": {
        "id": "QgMmmaJ_XzmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a softmax function and apply it on input_tensor to generate a probability for a multiclass classification task."
      ],
      "metadata": {
        "id": "xzKT8jsIYfY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor = torch.tensor([[1.0, -6.0, 2.5, -0.3, 1.2, 0.8]])\n",
        "\n",
        "# Create a softmax function and apply it on input_tensor\n",
        "softmax = nn.Softmax(dim=-1)\n",
        "probabilities = softmax(input_tensor)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "id": "QbNIg8SUYlNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a binary classifier in PyTorch\n",
        "Recall that a small neural network with a single linear layer followed by a sigmoid function is a binary classifier. It acts just like a logistic regression.\n",
        "\n",
        "In this exercise, you'll practice building this small network and interpreting the output of the classifier.\n",
        "\n",
        "The torch package and the torch.nn package have already been imported for you.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Create a neural network that takes a tensor of dimensions 1x8 as input, and returns an output of the correct shape for binary classification.\n",
        "Pass the output of the linear layer to a sigmoid, which both takes in and return a single float."
      ],
      "metadata": {
        "id": "saGNj387eEUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Implement a small neural network for binary classification\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(8, 1),\n",
        "  nn.Sigmoid()\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "KGU5juDweKyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From regression to multi-class classification\n",
        "Recall that the models we have seen for binary classification, multi-class classification and regression have all been similar, barring a few tweaks to the model.\n",
        "\n",
        "In this exercise, you'll start by building a model for regression, and then tweak the model to perform a multi-class classification.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Create a 4-layer linear neural network compatible with input_tensor as the input, and a regression value as output."
      ],
      "metadata": {
        "id": "ElY7zf5PeNk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Implement a neural network with exactly four linear layers\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(11, 20),\n",
        "  nn.Linear(20, 12),\n",
        "  nn.Linear(12, 6),\n",
        "  nn.Linear(6, 1)\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "DYWBQqdHeiY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update the network provided to perform a multi-class classification with four outputs."
      ],
      "metadata": {
        "id": "i1slHRm7gzqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_tensor = torch.Tensor([[3,4,6,7,10,12,2,3,6,8,9]])\n",
        "\n",
        "#Update network below to perform a multi-class classification with four labels\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(11, 20),\n",
        "    nn.Linear(20, 12),\n",
        "    nn.Linear(12, 6),\n",
        "    nn.Linear(6, 4),\n",
        "    nn.Softmax(dim=-1)\n",
        ")\n",
        "\n",
        "output = model(input_tensor)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "AxYnrIBlhdSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating one-hot encoded labels\n",
        "One-hot encoding is a technique that turns a single integer label into a vector of N elements, where N is the number of classes in your dataset. This vector only contains zeros and ones. In this exercise, you'll create the one-hot encoded vector of the label y provided.\n",
        "\n",
        "You'll practice doing this manually, and then make your life easier by leveraging the help of PyTorch! Your dataset contains three classes, and the class labels range from 0 to 2 (e.g., 0, 1, 2).\n",
        "\n",
        "NumPy is already imported as np, and torch.nn.functional as F. The torch package is also imported.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Manually create a one-hot encoded vector of the ground truth label y by filling in the NumPy array provided.\n",
        "Create a one-hot encoded vector of the ground truth label y using PyTorch."
      ],
      "metadata": {
        "id": "DNq29Z9L2Qw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  y = 1\n",
        "num_classes = 3\n",
        "\n",
        "# Create the one-hot encoded vector using NumPy\n",
        "one_hot_numpy = np.array([0, 1, 0])\n",
        "\n",
        "# Create the one-hot encoded vector using PyTorch\n",
        "one_hot_pytorch = F.one_hot(torch.tensor(y), num_classes)"
      ],
      "metadata": {
        "id": "GsMmAtNI2UWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating cross entropy loss\n",
        "Cross entropy loss is one of the most common ways to measure loss for classification problems. In this exercise, you will calculate cross entropy loss in PyTorch for a vector of predicted scores and a ground truth label. You are provided with the ground truth label y and the scores vector, a vector of model predictions before the final softmax function.\n",
        "\n",
        "You'll start by creating a one-hot encoded vector of the ground truth label y. Next, you'll instantiate a cross entropy loss function. Last, you'll call the loss function, which takes scores, and the one-hot encoded ground truth label, as inputs. Its output will be a single float, the loss of that sample.\n",
        "\n",
        "torch, CrossEntropyLoss, and torch.nn.functional as F have already been imported for you.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Create the one-hot encoded vector of the ground truth label y, with 4 features (one for each class), and assign it to one_hot_label."
      ],
      "metadata": {
        "id": "wZBVjgHh278-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "y = [2]\n",
        "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
        "\n",
        "# Create a one-hot encoded vector of the label y\n",
        "one_hot_label = F.one_hot(torch.tensor([2]), num_classes=4)"
      ],
      "metadata": {
        "id": "arornKI-2__h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "Create the cross entropy loss function and store it as criterion."
      ],
      "metadata": {
        "id": "GZBkrgdJ3L0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "y = [2]\n",
        "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
        "\n",
        "# Create a one-hot encoded vector of the label y\n",
        "one_hot_label = F.one_hot(torch.tensor(y), num_classes = scores.shape[1])\n",
        "\n",
        "# --- Code added ---\n",
        "# Create the cross entropy loss function\n",
        "criterion = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "WaGvxKln3W4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "Calculate the cross entropy loss using the one_hot_label vector and the scores vector, by calling the loss_function you created."
      ],
      "metadata": {
        "id": "1I6dElOd3sCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "y = [2]\n",
        "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
        "\n",
        "# Create a one-hot encoded vector of the label y\n",
        "one_hot_label = F.one_hot(torch.tensor(y), scores.shape[1])\n",
        "\n",
        "# Create the cross entropy loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# --- Code added ---\n",
        "# Calculate the cross entropy loss\n",
        "loss = criterion(scores.double(), one_hot_label.double())\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "nO8sJU5m3sm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accessing the model parameters\n",
        "A PyTorch model created with the nn.Sequential() is a module that contains the different layers of your network. Recall that each layer parameter can be accessed by indexing the created model directly. In this exercise, you will practice accessing the parameters of different linear layers of a neural network.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Access the weight parameter of the first linear layer.\n",
        "Access the bias parameter of the second linear layer."
      ],
      "metadata": {
        "id": "lydvT26C9rb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(16, 8),\n",
        "                      nn.Linear(8, 2)\n",
        "                     )\n",
        "\n",
        "# Access the weight of the first linear layer\n",
        "weight_0 = model[0].weight\n",
        "\n",
        "# Access the bias of the second linear layer\n",
        "bias_1 = model[1].bias"
      ],
      "metadata": {
        "id": "ZxEIAlmj9too"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updating the weights manually\n",
        "Now that you know how to access weights and biases, you will manually perform the job of the PyTorch optimizer. PyTorch functions can do what you're about to do, but it's helpful to do the work manually at least once, to understand what's going on under the hood.\n",
        "\n",
        "A neural network of three layers has been created and stored as the model variable. This network has been used for a forward pass and the loss and its derivatives have been calculated. A default learning rate, lr, has been chosen to scale the gradients when performing the update.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Create the gradient variables by accessing the local gradients of each weight tensor."
      ],
      "metadata": {
        "id": "9iuW_rpC-F8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight0 = model[0].weight\n",
        "weight1 = model[1].weight\n",
        "weight2 = model[2].weight\n",
        "\n",
        "# Access the gradients of the weight of each linear layer\n",
        "grads0 = model[0].weight.grad\n",
        "grads1 = model[1].weight.grad\n",
        "grads2 = model[2].weight.grad"
      ],
      "metadata": {
        "id": "1IS5VtOe-J0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Update the weights using the gradients scaled by the learning rate."
      ],
      "metadata": {
        "id": "Wjhk-fSf-U5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight0 = model[0].weight\n",
        "weight1 = model[1].weight\n",
        "weight2 = model[2].weight\n",
        "\n",
        "# --- Code edited ---\n",
        "# Access the gradients of the weight of each linear layer\n",
        "grads0 = weight0.grad\n",
        "grads1 = weight1.grad\n",
        "grads2 = weight2.grad\n",
        "\n",
        "# --- Code added ---\n",
        "# Update the weights using the learning rate and the gradients\n",
        "weight0 = weight0 - lr * grads0\n",
        "weight1 = weight1 - lr * grads1\n",
        "weight2 = weight2 - lr * grads2\n",
        "\n",
        "#lr is also given, but not in the code. The usual lr = 0.001"
      ],
      "metadata": {
        "id": "g25xtNv7-XFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the PyTorch optimizer\n",
        "In the previous exercise, you manually updated the weight of a network. You now know what's going on under the hood, but this approach is not scalable to a network of many layers.\n",
        "\n",
        "Thankfully, the PyTorch SGD optimizer does a similar job in a handful of lines of code. In this exercise, you will practice the last step to complete the training loop: updating the weights using a PyTorch optimizer.\n",
        "\n",
        "A neural network has been created and provided as the model variable. This model was used to run a forward pass and create the tensor of predictions pred. The one-hot encoded tensor is named target and the cross entropy loss function is stored as criterion.\n",
        "\n",
        "torch.optim as optim, and torch.nn as nn have already been loaded for you.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Use optim to create an SGD optimizer with a learning rate of your choice (must be less than one) for the model provided."
      ],
      "metadata": {
        "id": "fc78buJ2CVtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "xZ8B_7ydCYKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Update the model's parameters using the optimizer."
      ],
      "metadata": {
        "id": "INqPdtUwChhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- Added code ---\n",
        "loss = criterion(pred, target)\n",
        "loss.backward()\n",
        "\n",
        "# Update the model's parameters using the optimizer\n",
        "optimizer.step()"
      ],
      "metadata": {
        "id": "P2-BdcS4Ck9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the MSELoss\n",
        "For regression problems, we often use Mean Squared Error (MSE) as a loss function instead of cross-entropy. MSE calculates the squared difference between predicted values (y_pred) and actual values (y). In this exercise, you'll compute MSE loss using both NumPy and PyTorch.\n",
        "\n",
        "The torch package has been imported, along with numpy as np and torch.nn as nn.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the MSE loss using NumPy.\n",
        "Create a MSE loss function using PyTorch.\n",
        "Convert y_pred and y to tensors and then float data types, and then use them to calculate MSELoss using PyTorch as mse_pytorch."
      ],
      "metadata": {
        "id": "JM0_qnH1C_Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.array(10)\n",
        "y = np.array(1)\n",
        "\n",
        "# Calculate the MSELoss using NumPy\n",
        "mse_numpy = np.mean((y_pred - y)**2)\n",
        "\n",
        "# Create the MSELoss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Calculate the MSELoss using the created loss function\n",
        "mse_pytorch = criterion(torch.tensor(y_pred).float(), torch.tensor(y).float())\n",
        "print(mse_pytorch)"
      ],
      "metadata": {
        "id": "ir1NOI0yDB95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing a training loop\n",
        "In scikit-learn, the training loop is wrapped in the .fit() method, while in PyTorch, it's set up manually. While this adds flexibility, it requires a custom implementation.\n",
        "\n",
        "In this exercise, you'll create a loop to train a model for salary prediction.\n",
        "\n",
        "The show_results() function is provided to help you visualize some sample predictions.\n",
        "\n",
        "The package imports provided are: pandas as pd, torch, torch.nn as nn, torch.optim as optim, as well as DataLoader and TensorDataset from torch.utils.data.\n",
        "\n",
        "The following variables have been created: num_epochs, containing the number of epochs (set to 5); dataloader, containing the dataloader; model, containing the neural network; criterion, containing the loss function, nn.MSELoss(); optimizer, containing the SGD optimizer.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Write a for loop that iterates over the dataloader; this should be nested within a for loop that iterates over a range equal to the number of epochs.\n",
        "Set the gradients of the optimizer to zero."
      ],
      "metadata": {
        "id": "sV-PRvFzDOR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the number of epochs and then the dataloader\n",
        "for i in range(num_epochs):\n",
        "  for data in dataloader:\n",
        "    # Set the gradients to zero\n",
        "    optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "bD2EBsjsDSHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "Write the forward pass.\n",
        "Compute the MSE loss value using the criterion() function provided.\n",
        "Compute the gradients."
      ],
      "metadata": {
        "id": "5NPiU_UyDys3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the number of epochs and the dataloader\n",
        "for i in range(num_epochs):\n",
        "  for data in dataloader:\n",
        "    # Set the gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # --- Added code ---\n",
        "    # Run a forward pass\n",
        "    feature, target = data\n",
        "    prediction = model(feature)\n",
        "    # Calculate the loss\n",
        "    loss = criterion(prediction, target)\n",
        "    # Compute the gradients\n",
        "    loss.backward()"
      ],
      "metadata": {
        "id": "oltU97sgD1F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "Update the model's parameters."
      ],
      "metadata": {
        "id": "8JM4y7EaEakl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the number of epochs and the dataloader\n",
        "for i in range(num_epochs):\n",
        "  for data in dataloader:\n",
        "    # Set the gradients to zero\n",
        "    optimizer.zero_grad()\n",
        "    # Run a forward pass\n",
        "    feature, target = data\n",
        "    prediction = model(feature)\n",
        "    # Calculate the loss\n",
        "    loss = criterion(prediction, target)\n",
        "    # Compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # --- Added code ---\n",
        "    # Update the model's parameters\n",
        "    optimizer.step()\n",
        "show_results(model, dataloader)"
      ],
      "metadata": {
        "id": "D5MGTKkMEcxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing ReLU\n",
        "The rectified linear unit (or ReLU) function is one of the most common activation functions in deep learning.\n",
        "\n",
        "It overcomes the training problems linked with the sigmoid function you learned, such as the vanishing gradients problem.\n",
        "\n",
        "In this exercise, you'll begin with a ReLU implementation in PyTorch. Next, you'll calculate the gradients of the function.\n",
        "\n",
        "The nn module has already been imported for you.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Create a ReLU function in PyTorch."
      ],
      "metadata": {
        "id": "zKRDhRIv9yDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ReLU function with PyTorch\n",
        "relu_pytorch = nn.ReLU()"
      ],
      "metadata": {
        "id": "0MJs_AfG90XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Calculate the gradient of the ReLU function for x using the relu_pytorch() function you defined, then running a backward pass.\n",
        "Find the gradient at x."
      ],
      "metadata": {
        "id": "Qi_fZMNi-Plr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ReLU function with PyTorch\n",
        "relu_pytorch = nn.ReLU()\n",
        "\n",
        "# Apply your ReLU function on x, and calculate gradients\n",
        "x = torch.tensor(-1.0, requires_grad=True)\n",
        "y = relu_pytorch(x)\n",
        "y.backward()\n",
        "\n",
        "# Print the gradient of the ReLU function for x\n",
        "gradient = x.grad\n",
        "print(gradient)"
      ],
      "metadata": {
        "id": "eedVSuAt-QMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing leaky ReLU\n",
        "You've learned that ReLU is one of the most used activation functions in deep learning. You will find it in modern architecture. However, it does have the inconvenience of outputting null values for negative inputs and therefore, having null gradients. Once an element of the input is negative, it will be set to zero for the rest of the training. Leaky ReLU overcomes this challenge by using a multiplying factor for negative inputs.\n",
        "\n",
        "In this exercise, you will implement the leaky ReLU function in NumPy and PyTorch and practice using it. The numpy as np package, the torch package as well as the torch.nn as nn have already been imported.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Create a leaky ReLU function in PyTorch with a negative slope of 0.05.\n",
        "Call the function on the tensor x, which has already been defined for you"
      ],
      "metadata": {
        "id": "PBwVVHEY-yPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a leaky relu function in PyTorch\n",
        "leaky_relu_pytorch = nn.LeakyReLU(negative_slope=0.05)\n",
        "\n",
        "x = torch.tensor(-2.0)\n",
        "# Call the above function on the tensor x\n",
        "output = leaky_relu_pytorch(x)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "zEPlJFgQ-00M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting the number of parameters\n",
        "Deep learning models are famous for having a lot of parameters. Recent language models have billions of parameters. With more parameters comes more computational complexity and longer training times, and a deep learning practitioner must know how many parameters their model has.\n",
        "\n",
        "In this exercise, you will calculate the number of parameters in your model, first manually, and then using PyTorch.\n",
        "\n",
        "The torch.nn package has been imported as nn.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Question\n",
        "Calculate manually the number of parameters of the model below. How many does it have?\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "model = nn.Sequential(nn.Linear(16, 4),\n",
        "                      nn.Linear(4, 2),\n",
        "                      nn.Linear(2, 1))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "euMOAvTfQwpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(16, 4),\n",
        "                      nn.Linear(4, 2),\n",
        "                      nn.Linear(2, 1))\n",
        "\n",
        "total = 0\n",
        "for parameter in model.parameters():\n",
        "    total += parameter.numel()\n",
        "print(total)"
      ],
      "metadata": {
        "id": "-NL2MkaFQ2rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Now, confirm your manual calculation by iterating through the model's parameters to update the total variable with the total number of parameters in the model."
      ],
      "metadata": {
        "id": "VFvnP6jUQ_F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(nn.Linear(16, 4),\n",
        "                      nn.Linear(4, 2),\n",
        "                      nn.Linear(2, 1))\n",
        "\n",
        "total = 0\n",
        "\n",
        "# Calculate the number of parameters in the model\n",
        "for parameter in model.parameters():\n",
        "  total += parameter.numel()\n",
        "\n",
        "print(f\"The number of parameters in the model is {total}\")"
      ],
      "metadata": {
        "id": "iYLaA6RMRB2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manipulating the capacity of a network\n",
        "In this exercise, you will practice creating neural networks with different capacities. The capacity of a network reflects the number of parameters in said network. To help you, a calculate_capacity() function has been implemented, as follows:\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "def calculate_capacity(model):\n",
        "  total = 0\n",
        "  for p in model.parameters():\n",
        "    total += p.numel()\n",
        "  return total\n",
        "```\n",
        "This function returns the number of parameters in your model.\n",
        "\n",
        "The dataset you are training this network on has n_features features and n_classes classes. The torch.nn package has been imported as nn.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Create a 3-layer linear neural network with <120 parameters, using n_features as input and n_classes as output sizes.\n"
      ],
      "metadata": {
        "id": "ynpeAtS_Rf2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 8\n",
        "n_classes = 2\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Create a neural network with less than 120 parameters\n",
        "model = nn.Sequential(nn.Linear(n_features, 8),\n",
        "                      nn.Linear(8, 4),\n",
        "                      nn.Linear(4, n_classes))\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(calculate_capacity(model))"
      ],
      "metadata": {
        "id": "m6vPT-yLRwm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Create a 3-layer linear neural network with <120 parameters, using n_features as input and n_classes as output sizes."
      ],
      "metadata": {
        "id": "bnPkHQIeSVJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 8\n",
        "n_classes = 2\n",
        "\n",
        "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
        "\n",
        "# Create a neural network with more than 120 parameters\n",
        "model = nn.Sequential(nn.Linear(n_features, 16),\n",
        "                      nn.Linear(16, 8),\n",
        "                      nn.Linear(8, 3),\n",
        "                      nn.Linear(3, n_classes))\n",
        "\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(calculate_capacity(model))"
      ],
      "metadata": {
        "id": "9EZmz6qoSWEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with learning rate\n",
        "In this exercise, your goal is to find the optimal learning rate such that the optimizer can find the minimum of the non-convex function\n",
        " in ten steps.\n",
        "\n",
        "You will experiment with three different learning rate values. For this problem, try learning rate values between 0.001 to 0.1.\n",
        "\n",
        "You are provided with the optimize_and_plot() function that takes the learning rate for the first argument. This function will run 10 steps of the SGD optimizer and display the results.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "Try a small learning rate value such that the optimizer isn't able to get past the first minimum on the right."
      ],
      "metadata": {
        "id": "KT8KiPIif1DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a first learning rate value\n",
        "lr0 = 0.01\n",
        "optimize_and_plot(lr=lr0)"
      ],
      "metadata": {
        "id": "DPQVYo5gf21B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try a large learning rate value such that the optimizer skips past the global minimum at -2."
      ],
      "metadata": {
        "id": "l2QJNdpGf-mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a second learning rate value\n",
        "lr1 = 0.1\n",
        "optimize_and_plot(lr=lr1)"
      ],
      "metadata": {
        "id": "r3s_JkBegAiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the previous results, try a better learning rate value.\n",
        "\n"
      ],
      "metadata": {
        "id": "hyjU87afgWeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a third learning rate value\n",
        "lr2 = 0.09\n",
        "optimize_and_plot(lr=lr2)"
      ],
      "metadata": {
        "id": "riyGoILggZN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with momentum\n",
        "In this exercise, your goal is to find the optimal momentum such that the optimizer can find the minimum of the following non-convex function\n",
        " in 20 steps. You will experiment with two different momentum values. For this problem, the learning rate is fixed at 0.01.\n",
        "\n",
        "You are provided with the optimize_and_plot() function that accepts as input the momentum parameter. This function will run 20 steps of the SGD optimizer and display the results.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Try a first value for the momentum such that the optimizer gets stuck in the first minimum."
      ],
      "metadata": {
        "id": "6eM4SW2ogt5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a first value for momentum\n",
        "mom0 = 0\n",
        "optimize_and_plot(momentum=mom0)"
      ],
      "metadata": {
        "id": "bO_eskZLgwGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try a second value for the momentum such that the optimizer finds the global optimum."
      ],
      "metadata": {
        "id": "E4yESDOBg5Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try a second value for momentum\n",
        "mom1 = 0.92\n",
        "optimize_and_plot(momentum=mom1)"
      ],
      "metadata": {
        "id": "3TUQWAuVg6ni"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}